{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/tudor/Programs/anaconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tudor/Programs/anaconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tudor/Programs/anaconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tudor/Programs/anaconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tudor/Programs/anaconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tudor/Programs/anaconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tudor/Programs/anaconda3/envs/ML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tudor/Programs/anaconda3/envs/ML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tudor/Programs/anaconda3/envs/ML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tudor/Programs/anaconda3/envs/ML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tudor/Programs/anaconda3/envs/ML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tudor/Programs/anaconda3/envs/ML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from scipy import spatial\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "The goal of this project is to obtain the vector representations for words from text.\n",
    "\n",
    "The main idea is that words appearing in similar contexts have similar meanings. Because of that, word vectors of similar words should be close together. Models that use word vectors can utilize these properties, e.g., in sentiment analysis a model will learn that \"good\" and \"great\" are positive words, but will also generalize to other words that it has not seen (e.g. \"amazing\") because they should be close together in the vector space.\n",
    "\n",
    "Vectors can keep other language properties as well, like analogies. The question \"a is to b as c is to ...?\", where the answer is d, can be answered by looking into word vector space and calculating $\\mathbf{u}_b - \\mathbf{u}_a + \\mathbf{u}_c$, and finding the word vector that is the closest to the result.\n",
    "\n",
    "We are given a text that contains $N$ unique words $\\{ x_1, ..., x_N \\}$. We will focus on the Skip-Gram model in which the goal is to predict the context window $S = \\{ x_{i-l}, ..., x_{i-1}, x_{i+1}, ..., x_{i+l} \\}$ from current word $x_i$, where $l$ is the window size. \n",
    "\n",
    "We get a word embedding $\\mathbf{u}_i$ by multiplying the matrix $\\mathbf{U}$ with a one-hot representation $\\mathbf{x}_i$ of a word $x_i$. Then, to get output probabilities for context window, we multiply this embedding with another matrix $\\mathbf{V}$ and apply softmax. The objective is to minimize the loss: $-\\mathop{\\mathbb{E}}[P(S|x_i;\\mathbf{U}, \\mathbf{V})]$.\n",
    "\n",
    "You are given a dataset with positive and negative reviews. Your task is to:\n",
    "+ Construct input-output pairs corresponding to the current word and a word in the context window\n",
    "+ Implement forward and backward propagation with parameter updates for Skip-Gram model\n",
    "+ Train the model\n",
    "+ Test it on word analogies and sentiment analysis task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load data\n",
    "\n",
    "We'll be working with a subset of reviews for restaurants in Las Vegas. The reviews that we'll be working with are either 1-star or 5-star. You can download the used data set (`task03_data.npy`) from:\n",
    "\n",
    "* ([download link](https://syncandshare.lrz.de/dl/fi7cjApuE3Bd3xyfsyx3k9jr/task03_data.npy)) the preprocessed set of 1-star and 5-star reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.load(\"task_word2vec_data.npy\", allow_pickle=True)\n",
    "reviews_1star = [[x.lower() for x in s] for s in data.item()[\"reviews_1star\"]]\n",
    "reviews_5star = [[x.lower() for x in s] for s in data.item()[\"reviews_5star\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the vocabulary by taking the top 500 words by their frequency from both positive and negative sentences. We could also use the whole vocabulary, but that would be slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "vocabulary = [x for s in reviews_1star + reviews_5star for x in s]\n",
    "vocabulary, counts = zip(*Counter(vocabulary).most_common(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = len(vocabulary)\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Number of positive reviews: 1000\n",
      "Number of negative reviews: 2000\n",
      "Number of unique words: 500\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('Number of positive reviews:', len(reviews_1star))\n",
    "print('Number of negative reviews:', len(reviews_5star))\n",
    "print('Number of unique words:', VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to create two dictionaries: `word_to_ind` and `ind_to_word` so we can go from text to numerical representation and vice versa. The input into the model will be the index of the word denoting the position in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement\n",
    "---------\n",
    "word_to_ind: dict\n",
    "    The keys are words (str) and the value is the corresponding position in the vocabulary\n",
    "ind_to_word: dict\n",
    "    The keys are indices (int) and the value is the corresponding word from the vocabulary\n",
    "ind_to_freq: dict\n",
    "    The keys are indices (int) and the value is the corresponding count in the vocabulary\n",
    "\"\"\"\n",
    "\n",
    "word_to_ind = {}\n",
    "ind_to_word = {}\n",
    "ind_to_freq = {}\n",
    "\n",
    "for i,word in enumerate(vocabulary):\n",
    "    word_to_ind.update( {word : i} )\n",
    "    ind_to_word.update( {i : word} )\n",
    "    ind_to_freq.update( {i : counts[i]} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Word \"the\" is at position 0 appearing 2017 times\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('Word \\\"%s\\\" is at position %d appearing %d times' % \n",
    "      (ind_to_word[word_to_ind['the']], word_to_ind['the'], ind_to_freq[word_to_ind['the']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create word pairs\n",
    "\n",
    "We need all the word pairs $\\{ x_i, x_j \\}$, where $x_i$ is the current word and $x_j$ is from its context window. These will correspond to input-output pairs. We want them to be represented numericaly so you should use `word_to_ind` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_window(sentence, window_size):\n",
    "    sentence = [x for x in sentence if x in vocabulary]\n",
    "    pairs = []\n",
    "\n",
    "    \"\"\"\n",
    "    Iterate over all the sentences\n",
    "    Take all the words from (i - window_size) to (i + window_size) and save them to pairs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence: list\n",
    "        A list of sentences, each sentence containing a list of words of str type\n",
    "    window_size: int\n",
    "        A positive scalar\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pairs: list\n",
    "        A list of tuple (word index, word index from its context) of int type\n",
    "    \"\"\"\n",
    "    # we only need to look forward for our cooccurence matrix. We assume that the list should contain elements in both order, ie [cat dog] is not [dog cat]\n",
    "    encountered = {}\n",
    "    L = len(sentence)\n",
    "    #print('L of sentence is %s'%str(L))\n",
    "    for i, word in enumerate(sentence):\n",
    "        encountered[word] = True\n",
    "        for j in range(i+1,i+window_size+1):\n",
    "            if j< L and sentence[j] in encountered:\n",
    "                encountered[sentence[j]] = True\n",
    "                pairs.append((word_to_ind[word],word_to_ind[sentence[j]]))\n",
    "                pairs.append((word_to_ind[sentence[j]],word_to_ind[word]))\n",
    "                        \n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "First 5 pairs: [[338, 6], [6, 338], [372, 6], [6, 372], [90, 6]]\n",
      "Total number of pairs: 16810\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "data = []\n",
    "for x in reviews_1star + reviews_5star:\n",
    "    data += get_window(x, window_size=3)\n",
    "data = np.array(data)\n",
    "\n",
    "print('First 5 pairs:', data[:5].tolist())\n",
    "print('Total number of pairs:', data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate a weighting score to counter the imbalance between the rare and frequent words. Rare words will be sampled more frequently. See https://arxiv.org/pdf/1310.4546.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "probabilities = [1 - np.sqrt(1e-3 / ind_to_freq[x]) for x in data[:,0]]\n",
    "probabilities /= np.sum(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model definition\n",
    "\n",
    "In this part you should implement forward and backward propagation together with update of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Embedding():\n",
    "    def __init__(self, N, D, seed=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        N: int\n",
    "            Number of unique words in the vocabulary\n",
    "        D: int\n",
    "            Dimension of the word vector embedding\n",
    "        seed: int\n",
    "            Sets the random seed, if omitted weights will be random\n",
    "        \"\"\"\n",
    "\n",
    "        self.N = N\n",
    "        self.D = D\n",
    "        \n",
    "        self.init_weights(seed)\n",
    "    \n",
    "    def init_weights(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        \"\"\"\n",
    "        We initialize weight matrices U and V of dimension (D, N) and (N, D) respectively\n",
    "        \"\"\"\n",
    "        self.U = np.random.normal(0, np.sqrt(2 / self.D / self.N), (self.D, self.N))\n",
    "        self.V = np.random.normal(0, np.sqrt(2 / self.D / self.N), (self.N, self.D))\n",
    "\n",
    "    def one_hot(self, x, N):\n",
    "        \"\"\"\n",
    "        Given a vector returns a matrix with rows corresponding to one-hot encoding\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: array\n",
    "            M-dimensional vector containing integers from [0, N]\n",
    "        N: int\n",
    "            Number of posible classes\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        one_hot: array\n",
    "            (N, M) matrix where each column is N-dimensional one-hot encoding of elements from x \n",
    "        \"\"\"\n",
    "        M = x.shape[0]\n",
    "        one_hot = np.zeros((N,M))\n",
    "        one_hot[x,np.arange(M)] = 1\n",
    "        \n",
    "\n",
    "        assert one_hot.shape == (N, x.shape[0])\n",
    "        return one_hot\n",
    "\n",
    "    def loss(self, y, prob):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: array\n",
    "            (N, M) matrix of M samples where columns are one-hot vectors for true values\n",
    "        prob: array\n",
    "            (N, M) column of M samples where columns are probabily vectors after softmax\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: int\n",
    "            Cross-entropy loss calculated as: 1 / M * sum_i(sum_j(y_ij * log(prob_ij)))\n",
    "        \"\"\"\n",
    "        loss = -np.sum(y * np.log(prob)) / y.shape[1]\n",
    "        return loss\n",
    "    \n",
    "    def softmax(self, x, axis):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: array\n",
    "            A non-empty matrix of any dimension\n",
    "        axis: int\n",
    "            Dimension on which softmax is performed\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y: array\n",
    "            Matrix of same dimension as x with softmax applied to 'axis' dimension\n",
    "        \"\"\"\n",
    "\n",
    "        t = x - np.max(x,axis=axis)        \n",
    "        t = np.exp(t)\n",
    "        y = t/np.sum(t,axis)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def step(self, x, y, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Performs forward and backward propagation and updates weights\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: array\n",
    "            M-dimensional mini-batched vector containing input word indices of int type\n",
    "        y: array\n",
    "            Output words, same dimension and type as 'x'\n",
    "        learning_rate: float\n",
    "            A positive scalar determining the update rate\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "            Cross-entropy loss\n",
    "        d_U: array\n",
    "            Partial derivative of loss w.r.t. U\n",
    "        d_V: array\n",
    "            Partial derivative of loss w.r.t. V\n",
    "        \"\"\"\n",
    "        \n",
    "        # Input transformation\n",
    "        \"\"\"\n",
    "        Input is represented with M-dimensional vectors\n",
    "        We convert them to (N, M) matrices such that columns are one-hot \n",
    "        representations of the input\n",
    "        \"\"\"\n",
    "        x = self.one_hot(x, self.N)\n",
    "        y = self.one_hot(y, self.N)\n",
    "\n",
    "        \n",
    "        # Forward propagation\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        embedding: array\n",
    "            (D, M) matrix where columns are word embedding from U matrix\n",
    "        logits: array\n",
    "            (N, M) matrix where columns are output logits\n",
    "        prob: array\n",
    "            (N, M) matrix where columns are output probabilities\n",
    "        \"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        #assert: matrix U should have shape D,N\n",
    "        embedding = self.U @ x\n",
    "        #assert: matrix V should have shape N,D \n",
    "        logits = self.V @ embedding \n",
    "        \n",
    "        prob = self.softmax(logits, axis = 0)\n",
    "        \n",
    "        assert embedding.shape == (self.D, x.shape[1])\n",
    "        assert logits.shape == (self.N, x.shape[1])\n",
    "        assert prob.shape == (self.N, x.shape[1])\n",
    "    \n",
    "    \n",
    "        # Loss calculation\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        loss: int\n",
    "            Cross-entropy loss using true values and probabilities\n",
    "        \"\"\"\n",
    "        loss = self.loss(y,prob)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Backward propagation\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        d_U: array\n",
    "            (N, D) matrix of partial derivatives of loss w.r.t. U\n",
    "        d_V: array\n",
    "            (D, N) matrix of partial derivatives of loss w.r.t. V\n",
    "        \"\"\"\n",
    "        \n",
    "        d_logits = prob - y\n",
    "        d_V = d_logits @ embedding.transpose()\n",
    "        d_embedding = self.V.transpose() @ d_logits\n",
    "        d_U = d_embedding @ x.transpose()\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        assert d_V.shape == (self.N, self.D)\n",
    "        assert d_U.shape == (self.D, self.N)\n",
    "\n",
    "        \n",
    "        # Update the parameters\n",
    "        \"\"\"\n",
    "        Updates the weights with gradient descent such that W_new = W - alpha * dL/dW, \n",
    "        where alpha is the learning rate and dL/dW is the partial derivative of loss w.r.t. \n",
    "        the weights W\n",
    "        \"\"\"\n",
    "        self.U = self.U - learning_rate * d_U\n",
    "        self.V = self.V - learning_rate * d_V\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "        return loss, d_U, d_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Gradient check\n",
    "\n",
    "The following code checks whether the updates for weights are implemented correctly. It should run without an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Gradients checked - all good!\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def get_loss(model, old, variable, epsilon, x, y, i, j):\n",
    "    delta = np.zeros_like(old)\n",
    "    delta[i, j] = epsilon\n",
    "\n",
    "    model.init_weights(seed=132) # reset weights\n",
    "    setattr(model, variable, old + delta) # change one weight by a small amount\n",
    "    loss, _, _ = model.step(x, y) # get loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "def gradient_check_for_weight(model, variable, i, j, k, l):\n",
    "    x, y = np.array([i]), np.array([j]) # set input and output\n",
    "    \n",
    "    old = getattr(model, variable)\n",
    "    \n",
    "    model.init_weights(seed=132) # reset weights\n",
    "    _, d_U, d_V = model.step(x, y) # get gradients with backprop\n",
    "    grad = { 'U': d_U, 'V': d_V }\n",
    "    \n",
    "    eps = 1e-4\n",
    "    loss_positive = get_loss(model, old, variable, eps, x, y, k, l) # loss for positive change on one weight\n",
    "    loss_negative = get_loss(model, old, variable, -eps, x, y, k, l) # loss for negative change on one weight\n",
    "    \n",
    "    \n",
    "    true_gradient = (loss_positive - loss_negative) / 2 / eps # calculate true derivative wrt one weight\n",
    "\n",
    "    assert abs(true_gradient - grad[variable][k, l]) < 1e-5 # require that the difference is small\n",
    "\n",
    "def gradient_check():\n",
    "    N, D = VOCABULARY_SIZE, EMBEDDING_DIM\n",
    "    model = Embedding(N, D)\n",
    "\n",
    "    # check for V\n",
    "    for _ in range(20):\n",
    "        i, j, k = [np.random.randint(0, d) for d in [N, N, D]] # get random indices for input and weights\n",
    "        gradient_check_for_weight(model, 'V', i, j, i, k)\n",
    "\n",
    "    # check for U\n",
    "    for _ in range(20):\n",
    "        i, j, k = [np.random.randint(0, d) for d in [N, N, D]]\n",
    "        gradient_check_for_weight(model, 'U', i, j, k, i)\n",
    "\n",
    "    print('Gradients checked - all good!')\n",
    "\n",
    "gradient_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our model using stochastic gradient descent. At every step we sample a mini-batch from data and update the weights.\n",
    "\n",
    "The following function samples words from data and creates mini-batches. It subsamples frequent words based on previously calculated probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(data, size, prob):\n",
    "    i = np.random.choice(data.shape[0], size, p=prob)\n",
    "    return data[i, 0], data[i, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model can take some time so plan accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "shape of data is (16810, 2)\n",
      "shape of probabilities is (16810,)\n",
      "Starting Training..\n",
      "Iteration: 10000 Loss: 4.8415865030486955\n",
      "Iteration: 20000 Loss: 4.2938475772364395\n",
      "Iteration: 30000 Loss: 4.145205140959426\n",
      "Iteration: 40000 Loss: 4.033933901790048\n",
      "Iteration: 50000 Loss: 3.9394818661218\n",
      "Iteration: 60000 Loss: 3.8602289300511017\n",
      "Iteration: 70000 Loss: 3.7844069119971158\n",
      "Iteration: 80000 Loss: 3.7199381413927752\n",
      "Iteration: 90000 Loss: 3.662827417324268\n",
      "Iteration: 100000 Loss: 3.6143726197844006\n",
      "Iteration: 110000 Loss: 3.5767620645120566\n",
      "Iteration: 120000 Loss: 3.5468948516771954\n",
      "Iteration: 130000 Loss: 3.5226116833054766\n",
      "Iteration: 140000 Loss: 3.5074833745265424\n",
      "Iteration: 150000 Loss: 3.4949006183311337\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = Embedding(N=VOCABULARY_SIZE, D=EMBEDDING_DIM)\n",
    "\n",
    "losses = []\n",
    "\n",
    "MAX_ITERATIONS = 150000\n",
    "PRINT_EVERY =10000\n",
    "\n",
    "print('shape of data is %s'%str(data.shape))\n",
    "print('shape of probabilities is %s'%str(probabilities.shape))\n",
    "\n",
    "print(\"Starting Training..\")\n",
    "\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    x, y = get_batch(data, 128, probabilities)\n",
    "    loss, _, _ = model.step(x, y, 1e-3)\n",
    "    losses.append(loss)\n",
    "\n",
    "    if (i + 1) % PRINT_EVERY == 0:\n",
    "        print('Iteration:', i + 1, 'Loss:', np.mean(losses[-PRINT_EVERY:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding matrix is given by $\\mathbf{U}^T$, where the $i$th row is the vector for $i$th word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(500, 100)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "emb_matrix = model.U.T\n",
    "print (emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Analogies\n",
    "\n",
    "As mentioned before, vectors can keep some language properties like analogies. Given a relation a:b and a query c, we can find d such that c:d follows the same relation. We hope to find d by using vector operations. In this case, finding the real word vector $\\mathbf{u}_d$ closest to $\\mathbf{u}_b - \\mathbf{u}_a + \\mathbf{u}_c$ gives us d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "go is to going as come is to [come|going|strip|disappointed|time]\n",
      "look is to looking as come is to [come|looking|disappointed|night|me]\n",
      "you is to their as we is to [we|their|friend|pancakes|about]\n",
      "what is to that as when is to [that|when|wine|dishes|he]\n",
      "go is to went as is is to [is|spicy|tasty|nothing|must]\n",
      "go is to went as find is to [find|least|went|having|15]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def cosine_distance(v1,v2):\n",
    "    \"\"\"Compute cosine distance between two vectors\n",
    "        \n",
    "    \"\"\"\n",
    "    d = 1.0 - np.dot(v1,v2.T)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    return d\n",
    "\n",
    "triplets = [['go', 'going', 'come'], ['look', 'looking', 'come'], ['you', 'their', 'we'], \n",
    "            ['what', 'that', 'when'], ['go', 'went', 'is'], ['go', 'went', 'find']]\n",
    "\n",
    "for triplet in triplets:\n",
    "    a, b, c = triplet\n",
    "\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    candidates: list\n",
    "        A list of 5 closest words, measured with cosine similarity, to the vector u_b - u_a + u_c\n",
    "    \"\"\"\n",
    "    \n",
    "    #this could be done more efficienly, with hashing methods ..\n",
    "    #convert words to vector representation\n",
    "    a = model.one_hot(np.array(word_to_ind[a]).reshape(1,1),VOCABULARY_SIZE)\n",
    "    b = model.one_hot(np.array(word_to_ind[b]).reshape(1,1),VOCABULARY_SIZE)\n",
    "    c = model.one_hot(np.array(word_to_ind[c]).reshape(1,1),VOCABULARY_SIZE)\n",
    "    vect = (b-a+c).T@emb_matrix\n",
    "    closest = [] # closeste is a list containig lists of 2 elem. First elem is word, second elem is cosine_disatance of that word towards 'vect'\n",
    "    for index in range(VOCABULARY_SIZE):\n",
    "        embedded_word = model.one_hot(np.array(index).reshape(1,1), VOCABULARY_SIZE).T@emb_matrix\n",
    "        dist  = cosine_distance(vect,embedded_word)\n",
    "        if len(closest) <5 or dist<closest[4][1]:\n",
    "            for j in range(5):\n",
    "                \n",
    "                if j+1>len(closest) or dist<closest[j][1]:\n",
    "                    closest.insert(j,[ind_to_word[index],dist])\n",
    "                    break\n",
    "    \n",
    "    #print(closest[:5])\n",
    "    candidates = [i[0] for i in closest[:5]]   \n",
    "            \n",
    "    \n",
    "    a, b, c = triplet\n",
    "    print('%s is to %s as %s is to [%s]' % (a, b, c, '|'.join(candidates)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "Our end goal is to use the pretrained word vectors on some downstream task, e.g. sentiment analysis. We first generate a dataset where we just concatenate 1 and 5-star reviews into `all_sentences`. We also create a list `Y` with labels 1 for positive reviews and 0 for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "all_sentences = reviews_1star + reviews_5star\n",
    "Y = np.array([0] * len(reviews_1star) + [1] * len(reviews_5star))\n",
    "\n",
    "SENTENCES_SIZE = len(all_sentences)\n",
    "MAX_SENTENCE_LENGTH = max([len(x) for x in all_sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to create an array $\\mathbf{X}$ where (i,j,k) element denotes $k$th value of an embedding for $j$th word in $i$th sentence in the dataset. In addition, we need a list that keeps track of how many words are in each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[[-7.58255921e-01  7.43771913e-01 -5.35518494e-01  2.11012910e-01\n",
      "   -1.65406892e-02  2.93474367e-01  1.27429515e-01 -5.61768491e-02\n",
      "    2.68214102e-01  1.35552517e-01 -4.54509625e-01  7.65691170e-02\n",
      "    3.74128495e-01  7.16447522e-01  7.71596098e-02  8.43460634e-03\n",
      "    5.27289902e-01  2.80363945e-01  2.07284782e-01 -1.51031270e-01\n",
      "   -8.14024756e-01  5.08827327e-01 -2.76994516e-02  7.71715056e-02\n",
      "   -5.18355786e-02 -2.66884350e-01  1.49624949e+00 -1.10841377e+00\n",
      "   -7.09744595e-02  7.96734812e-01 -2.98296961e-01  8.31113283e-01\n",
      "   -1.50502861e-01 -1.11521997e+00  7.92683191e-01 -1.98340520e-01\n",
      "   -1.16317328e-01  4.98666755e-01 -8.52701494e-01 -3.55726099e-01\n",
      "   -6.11058303e-01 -5.79050587e-01 -8.33480603e-01 -4.83956797e-01\n",
      "   -6.27022781e-01  1.40087394e+00  3.18774039e-01  7.49836895e-01\n",
      "   -6.83035163e-02  1.26848699e-01 -3.37920366e-01  3.41090374e-01\n",
      "    7.85091590e-01 -3.62430432e-01  8.48497022e-02  2.88470359e-01\n",
      "   -9.79708033e-01  2.98556873e-01 -9.16640423e-01  7.59806435e-01\n",
      "   -5.41260824e-01  1.64018421e-01 -4.33716820e-01  2.54649620e-01\n",
      "    7.82807226e-01 -4.12151456e-01 -6.18520365e-02 -2.74644151e-01\n",
      "   -1.80722074e-01 -1.02013661e+00 -2.44817257e-01 -8.75485367e-01\n",
      "   -2.28001522e-01  5.04544932e-02  5.87878518e-01  2.13801200e-01\n",
      "    1.44954392e-01 -1.33869182e-01  5.40035733e-01 -1.21840961e+00\n",
      "    5.77712381e-01  2.24632367e-01 -9.79130368e-01 -3.96181939e-01\n",
      "   -4.81691790e-01  2.94758439e-01 -1.12417443e+00  4.36899423e-01\n",
      "   -9.12498864e-02  7.13559023e-01 -6.97902483e-01  6.10890484e-01\n",
      "   -7.74982310e-01 -8.48837122e-02 -1.09734843e-01 -1.57084665e-01\n",
      "    5.41383957e-01  4.01766087e-01  1.77314599e-01  1.07130992e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [-1.14506495e+00 -1.11030261e-02  1.34259300e-02 -3.52348629e-01\n",
      "   -1.19198606e+00 -2.60561623e-01  1.43185151e-01 -4.54735197e-01\n",
      "   -4.11121141e-01 -7.20472724e-01  1.80377179e-01  7.42881901e-01\n",
      "    1.17901482e-01  7.83961737e-02  3.44272008e-01  6.75412480e-01\n",
      "    3.86763286e-01  7.50047253e-02 -5.09583182e-01  1.18488622e+00\n",
      "   -4.11095341e-01  2.59906298e-01  4.62356084e-01  3.71483922e-01\n",
      "   -9.97873942e-01 -1.05827239e+00 -2.07779974e-01 -7.63287338e-01\n",
      "   -2.71003557e-01  6.47696182e-02 -4.05356363e-01 -7.07558022e-01\n",
      "   -4.79665159e-01 -5.41621737e-01  4.13114337e-01 -3.04511612e-01\n",
      "    7.07481289e-01  9.15313195e-02  2.84882438e-01  1.38300580e-01\n",
      "    1.45056220e+00  5.74494919e-01  4.60073779e-01 -3.17277319e-01\n",
      "   -4.96595062e-01  2.01986782e-01  7.74518458e-01  7.57064293e-01\n",
      "   -1.46318885e+00  2.54259196e-01 -1.48670799e-01  2.09287972e-01\n",
      "   -5.81125065e-02  7.98536415e-01 -3.21394752e-01 -5.32092561e-01\n",
      "    2.99719928e-02 -3.20798419e-01 -1.41595207e-01 -5.35483711e-01\n",
      "   -6.64361400e-01  1.20227917e+00 -5.46297059e-01  2.05858730e-01\n",
      "    1.92191558e+00  7.91063532e-01 -3.91755636e-01  3.94838716e-01\n",
      "    7.06147844e-02  1.94719896e-01 -2.14356436e-01 -5.70578526e-02\n",
      "    4.99378360e-01 -4.25031116e-01  2.06949555e-01 -3.73850845e-01\n",
      "   -1.33984219e-01  1.06015445e-02 -1.46272028e+00 -8.92012108e-01\n",
      "    5.76074926e-01 -9.74896550e-03 -1.57094453e-01  5.72002060e-01\n",
      "   -7.71049392e-01  3.08992151e-01 -4.80556572e-03  8.10504395e-04\n",
      "    9.13841499e-02  5.40602050e-01 -3.41239224e-01  6.17082274e-01\n",
      "    1.55693082e-01  3.16099121e-01  4.20343023e-01 -2.54488015e-02\n",
      "    2.97785735e-01  5.96978685e-01  4.57309765e-01  2.52244441e-01]\n",
      "  [ 2.27669419e-01  5.84668930e-01  3.79427688e-01 -1.70560167e-01\n",
      "   -5.82057688e-01 -1.55763594e-01 -2.15336762e-02 -1.40800477e-01\n",
      "   -9.96999396e-02 -2.79370029e-01 -2.23331287e-01  4.28376623e-01\n",
      "    1.11404095e-01  3.67933334e-02  1.96808618e-01 -1.02867756e-01\n",
      "   -1.72030082e-01 -3.90748165e-01 -5.89538370e-01  1.68466138e-01\n",
      "    1.86534360e-01 -1.70737342e-01  2.18518876e-01 -2.55394100e-01\n",
      "    3.66586545e-01 -4.45483414e-01 -2.25059570e-01  1.74519987e-01\n",
      "   -1.44234848e-01  3.79048767e-01 -5.48794758e-01 -4.47114762e-01\n",
      "   -6.28469314e-01 -1.51147214e-01 -1.31519536e-01 -5.00808204e-01\n",
      "    4.01683946e-01  4.22203871e-01 -9.67815398e-02 -4.27018757e-01\n",
      "   -3.91116322e-01  1.78134718e-01 -1.83121787e-02 -1.47114064e-01\n",
      "   -1.99374599e-02  6.86802262e-02  7.47067454e-01  2.09100133e-01\n",
      "   -6.02206798e-01 -1.36906137e-01 -2.33096368e-01 -4.37052697e-01\n",
      "   -3.29933011e-01 -4.15684754e-01  3.10107402e-01  1.11159158e-01\n",
      "    1.81258348e-01 -1.72959934e-01 -4.16368908e-01 -2.65731791e-01\n",
      "   -2.16829207e-01  6.40986085e-01  2.47946025e-01  2.01974862e-01\n",
      "    9.09170431e-02  2.77228212e-01  2.04169160e-01  1.44887025e-01\n",
      "    3.16794758e-01 -3.80088593e-01 -2.37671241e-01 -2.25732831e-01\n",
      "    8.91734763e-03  7.63985760e-02  1.04491862e-01  9.55864874e-01\n",
      "    6.10298598e-01  2.52009509e-01  2.27942534e-01  3.09523940e-02\n",
      "   -3.96745293e-02 -3.30039512e-01 -2.75093316e-01 -4.07964273e-01\n",
      "   -2.47019865e-01  2.77914795e-01  2.61648780e-01 -9.45537733e-02\n",
      "    2.86306078e-01  5.55082079e-01 -4.74815697e-01  3.40103824e-01\n",
      "    3.44893913e-01  1.57410243e-01 -3.08793456e-01  5.65463351e-01\n",
      "    6.82871383e-02 -2.45171476e-02 -5.23851306e-01  4.44947015e-01]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [-2.65384011e-01 -1.32161767e-01  6.71199075e-02 -1.56649215e-01\n",
      "    8.03706904e-02 -3.71252086e-01 -9.61803397e-02 -4.12114626e-01\n",
      "    1.76240602e-01  1.86336574e-02 -1.53720160e-01 -2.34671822e-01\n",
      "    3.30863877e-01 -2.43932421e-01  1.52688317e-01  5.50177308e-02\n",
      "    1.39666134e-01  2.08208109e-01 -2.10571392e-01  6.51455671e-02\n",
      "   -1.54628408e-01  3.91549477e-01  4.13928224e-01 -7.49419449e-02\n",
      "   -2.77909001e-01 -3.20153316e-01 -1.76195007e-01  1.07816883e-01\n",
      "   -4.27584583e-01 -1.65372957e-01 -2.88789299e-03  3.09159779e-01\n",
      "   -5.33936244e-01 -2.49613791e-01 -3.16683205e-02 -4.72276144e-01\n",
      "   -1.62265612e-01  4.47003480e-01  8.72474616e-02  7.48614785e-02\n",
      "   -5.62436433e-02  3.46171903e-01  2.25502856e-01 -2.47028869e-01\n",
      "    4.30194327e-02  2.26434654e-01  1.04120536e-01 -2.56825372e-01\n",
      "   -3.63808506e-01 -1.31188923e-01  1.42237152e-01 -9.10379928e-02\n",
      "    4.89770927e-02  4.94311498e-01 -3.07063071e-02  2.16843410e-01\n",
      "   -3.41933137e-02  1.59636665e-01  2.20217836e-01  2.00081014e-01\n",
      "    6.07877211e-03  4.87583761e-02  2.99412587e-01 -1.31307161e-01\n",
      "    9.59923953e-02  2.03400695e-01  1.87586131e-01  9.84639364e-02\n",
      "    4.72145276e-01 -6.74898321e-02  1.58420026e-01  1.56859974e-02\n",
      "    2.87481197e-01  1.15366035e-01 -1.55773848e-01  2.38845120e-01\n",
      "    3.30365487e-01  4.14318085e-02  1.42289552e-01 -7.49074517e-02\n",
      "    2.57486961e-01  1.81259285e-01 -2.89687144e-01 -2.71691236e-01\n",
      "    1.85822242e-01  2.61300303e-01  5.18917515e-01 -1.19073280e-01\n",
      "   -9.94656118e-02  4.21816736e-01  1.74580811e-02 -1.09604532e-01\n",
      "    3.14461838e-01  1.31301458e-02 -2.44190624e-03 -2.18928683e-01\n",
      "    4.32974760e-01  7.34260244e-02  3.64668204e-04 -1.00244852e-01]\n",
      "  [-2.91103264e-02  1.27988698e+00 -6.01759805e-01  3.89731148e-01\n",
      "   -4.75038415e-01  7.91120921e-02  5.55589046e-01 -5.38832997e-01\n",
      "   -4.26260560e-01  1.40667105e-01 -7.01313489e-01  1.28727556e+00\n",
      "    2.08390763e-01 -1.71310579e-01 -2.99778160e-01 -4.07843055e-01\n",
      "    3.41877450e-01  1.90477960e-01  1.79599231e-01  4.02473492e-01\n",
      "    6.50425277e-02  4.80878963e-01  1.13175454e-01  3.34335084e-01\n",
      "    2.02776249e-01  3.33034423e-01  1.48528781e+00 -2.79946339e-01\n",
      "    3.55725195e-02 -4.52728498e-01  9.25157501e-01  5.09284119e-01\n",
      "    1.74698968e-01 -2.94077578e-01  1.64974142e-01 -7.35184562e-01\n",
      "   -7.14309504e-02  9.83057159e-01  5.52230064e-01 -3.72435004e-01\n",
      "    5.67894889e-01  4.46276160e-01  1.91780038e-01 -1.37234175e+00\n",
      "    7.48172277e-01  2.07049002e-01 -3.88782213e-02 -5.25874430e-02\n",
      "    7.20802094e-01  2.02657509e-01  1.05925851e-01 -5.03091752e-01\n",
      "    4.90989933e-01  3.90170032e-01  8.28122125e-01 -6.94721963e-01\n",
      "   -2.46196646e-01  6.78203086e-02  8.46060432e-01 -4.09898750e-02\n",
      "    7.43639676e-01 -5.88648802e-01 -5.36779774e-01  1.33388048e-01\n",
      "   -5.85475694e-01  7.18019316e-01 -2.45752504e-01  8.40202078e-01\n",
      "    2.60236495e-01 -7.96578273e-01  4.26709140e-01 -9.45716377e-01\n",
      "   -4.57059092e-02  7.37100305e-01  4.35173603e-02  7.54058793e-02\n",
      "   -8.30591770e-01 -5.01284370e-01 -1.69335184e-01  8.19664109e-01\n",
      "   -2.70141373e-02  4.38023958e-01 -3.60017428e-01 -1.55370304e+00\n",
      "    5.84119712e-01  5.06872330e-01  9.30881900e-01  2.63698826e-01\n",
      "    5.43247554e-01  8.96076873e-01 -2.73648854e-01 -1.81272711e-01\n",
      "    7.58448320e-01  1.87931994e-01  1.29935860e-01  3.27294277e-01\n",
      "    7.06714764e-01 -6.86769581e-02  5.79660626e-02  9.55653251e-02]\n",
      "  [-5.71672308e-02 -7.17393653e-02  1.50164884e-01  5.01363051e-02\n",
      "   -2.49970969e-01 -7.10476377e-01 -1.13122519e-01 -5.31792602e-01\n",
      "    1.27481356e-02  4.93083970e-01 -4.76729310e-01 -4.41879918e-01\n",
      "    3.67437178e-01 -8.76699979e-02  4.78294271e-01  3.89746672e-01\n",
      "    2.00962068e-01  4.43414024e-01 -3.37214944e-01  4.38170058e-01\n",
      "   -4.17215825e-01  4.25491368e-01  4.39483382e-01 -6.61482487e-02\n",
      "   -2.78115803e-01 -1.97069632e-01  2.11034566e-01  4.14036506e-02\n",
      "   -3.57007139e-01 -2.13460251e-01 -3.44141309e-01  3.00582253e-01\n",
      "   -4.01318854e-01 -3.95782671e-01  3.04244413e-01 -3.10417470e-01\n",
      "   -3.15396629e-01  3.22696387e-01  3.03398643e-01 -6.58378144e-01\n",
      "   -1.94633050e-01  1.34917111e-01 -9.37970939e-02 -3.81526133e-02\n",
      "    1.66979956e-01  4.62149610e-01  1.52349157e-01 -2.07168058e-01\n",
      "   -3.91988935e-01 -4.12995859e-02  1.69061930e-01 -1.44372989e-01\n",
      "    2.35901451e-01  3.85915982e-01 -9.78009415e-02  5.99984636e-01\n",
      "   -4.65475406e-01  2.33379595e-01  2.99927504e-01 -6.66817647e-02\n",
      "    2.66224349e-01  2.33654595e-01  3.70423261e-01 -1.52087988e-01\n",
      "    1.63644961e-01 -2.25491053e-01  2.56244477e-01 -1.94174664e-01\n",
      "    3.22655090e-01 -4.69175375e-01  9.73682326e-02 -1.52666324e-01\n",
      "    9.77115736e-02  7.36901724e-01 -3.97455787e-01  3.30147222e-01\n",
      "    3.06717347e-01 -3.59451697e-01  3.13391210e-01 -5.77678646e-01\n",
      "    2.56617812e-01  4.90410653e-02 -5.66070535e-01  2.09639603e-01\n",
      "   -1.87540673e-01  5.36632953e-01 -5.86064031e-02 -3.33642403e-01\n",
      "   -3.89894379e-02  7.73671691e-01 -3.33833471e-01 -3.83474990e-02\n",
      "    5.73300926e-03  1.13266363e-01  9.84598299e-02 -1.28471837e-01\n",
      "   -6.73541712e-02  1.32730787e-01  1.80184168e-01 -1.78331289e-01]\n",
      "  [-8.99169542e-02 -8.28917965e-02 -4.25458994e-01  7.69759210e-01\n",
      "    1.64886459e-01 -3.51313054e-01 -3.04280051e-02  1.00288357e+00\n",
      "   -4.08053623e-01 -2.70000021e-01  4.95671953e-02 -1.41411857e-01\n",
      "   -1.58067679e-01  1.63750162e-01  2.41886290e-01 -8.52712366e-01\n",
      "   -3.90135021e-01  5.21417418e-02 -2.36550633e-01  2.72508670e-01\n",
      "   -5.50721702e-01  1.09691039e+00 -6.86087085e-01 -1.56103768e-01\n",
      "   -1.58630812e-01  3.19516181e-01  1.44430887e+00  1.03468484e-01\n",
      "   -7.34344116e-03  2.62626526e-01 -2.97342859e-01 -1.26107846e-01\n",
      "   -1.68010886e-01 -1.65282893e-01 -3.28826992e-01  5.65297752e-02\n",
      "    1.40711693e-02 -1.88476147e-01 -1.31278896e+00  6.87334379e-01\n",
      "    2.19078830e-01  1.96192902e-01  2.50335647e-01 -1.20550134e-01\n",
      "    1.93181034e-01  1.35008413e-01 -6.52113070e-01  3.10502252e-01\n",
      "   -3.57009954e-01  2.34732872e-01  5.48960071e-01 -3.36699112e-01\n",
      "   -4.14209246e-01 -4.79500212e-01 -4.28364826e-02 -5.20155401e-02\n",
      "   -4.35507011e-01  2.15384268e-01  4.09478720e-01  8.31362205e-01\n",
      "    2.03983716e-01 -1.60569271e-01 -1.98550362e-01 -6.86038674e-01\n",
      "    2.67852790e-01  9.75226081e-01 -9.37894286e-02 -3.20071164e-01\n",
      "    1.35048911e+00  3.04111725e-02  1.92100069e-01  7.23482887e-01\n",
      "    5.82135211e-01  5.63057630e-02  6.13806847e-01 -1.13851033e-01\n",
      "   -4.76441138e-01 -1.65494964e-01 -2.77110929e-01  4.16173503e-02\n",
      "    8.78633184e-01  4.88366853e-03 -1.11151752e+00  5.16126659e-01\n",
      "    1.38454273e+00  1.13544605e+00  1.27389119e-01 -6.57683731e-01\n",
      "   -2.29281954e-01  6.73058881e-01  2.77701420e-01 -5.32324499e-01\n",
      "    8.14950277e-01  6.81230517e-01  6.34876141e-01 -1.13984768e+00\n",
      "   -3.94168601e-01 -5.00294582e-01  1.07214750e+00 -3.48035377e-01]\n",
      "  [ 3.58464375e-01  1.33155829e-01 -3.96355576e-03  3.89018297e-01\n",
      "   -3.75585247e-01  1.69939545e-01 -8.81330911e-03  2.55931520e-01\n",
      "    1.34570500e-01  3.61634004e-02  4.53241491e-01 -6.31216295e-02\n",
      "   -1.21833842e-02 -9.06158313e-02  1.51407116e-01  3.46695699e-01\n",
      "    8.82390956e-01 -6.54706331e-02  2.04376446e-02  2.68373056e-01\n",
      "   -1.56754117e-01  8.65393453e-01  2.09460747e-01  2.16560767e-01\n",
      "   -3.43491579e-02 -1.81957328e-01  1.73397367e-01  3.04115147e-01\n",
      "    7.33745516e-02 -7.64610235e-02  3.34727298e-01  2.44219131e-01\n",
      "   -4.11324831e-01  6.46438569e-02  2.92015176e-01 -2.63915107e-01\n",
      "    1.22374769e-01 -6.35137025e-02  4.35290786e-02 -1.91842508e-01\n",
      "    5.44695704e-01  6.43302468e-02  1.08292030e-01  6.13288587e-01\n",
      "    1.82439253e-01  6.76335827e-01 -1.55291927e-01  6.51325155e-03\n",
      "   -4.30451747e-01 -3.33378921e-01  1.64407186e-01 -3.14540859e-01\n",
      "    2.61404488e-01 -2.51186769e-01 -7.30153064e-03  2.28262963e-01\n",
      "   -1.58305390e-01  1.76628371e-01  5.88363780e-01  1.89805534e-01\n",
      "   -5.18307772e-01  1.00822399e-01 -2.54501305e-01 -2.74729556e-01\n",
      "   -8.04788922e-02  8.62948597e-02  2.11735572e-01  2.28720612e-01\n",
      "    3.02912248e-01 -1.53704098e-01  7.63415939e-01  9.55761903e-02\n",
      "    3.38508419e-01  3.45779216e-01  1.37525523e-01  2.40394012e-01\n",
      "   -6.29749845e-01 -3.06004341e-01 -2.62192090e-01 -7.35397692e-01\n",
      "    7.22281315e-02  1.71901503e-01  4.54087991e-01 -1.43809642e-02\n",
      "    1.96404279e-02  6.65631452e-01  1.32250099e-01  7.30928451e-02\n",
      "   -2.03672516e-01  5.67679451e-01  1.83038508e-01 -5.14482521e-01\n",
      "    2.04578187e-01 -7.69005954e-03  4.02932188e-02  2.28692541e-02\n",
      "   -1.13631755e-01  1.50239059e-01  2.01540338e-01  1.28419632e-01]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 5.60213189e-02  1.04396383e-01 -1.44836234e-01  1.43917964e-01\n",
      "   -3.26295625e-01 -2.35767343e-01 -8.63829526e-02 -2.08958770e-01\n",
      "    5.50006075e-02  3.14856886e-01  1.39885836e-01 -1.04506051e-01\n",
      "   -1.20685459e-01 -2.54596089e-01 -6.42343165e-03  6.55530455e-02\n",
      "   -2.97713453e-02 -1.76488049e-02  4.51039686e-02  5.00547162e-01\n",
      "   -2.09753945e-01  1.08379798e-01 -3.18293774e-02  5.10300011e-05\n",
      "   -3.66585587e-01 -3.33310799e-01  2.59124371e-01 -9.21508027e-02\n",
      "    5.55508129e-02 -1.10787561e-01  1.86715832e-01  2.13829451e-01\n",
      "    1.23036106e-01 -3.44408897e-01 -8.14873039e-02 -3.44827046e-01\n",
      "    1.70089958e-01 -3.34645076e-02  6.50077858e-02 -2.93085049e-01\n",
      "    2.67659713e-01 -4.92407929e-02 -2.80159212e-02  1.02706538e-01\n",
      "    3.63417392e-01  3.53494054e-01 -2.36450827e-01 -1.04404574e-01\n",
      "   -3.67840988e-01  1.72093566e-01  4.85603272e-01  1.37453263e-01\n",
      "    4.11270275e-01 -1.44108620e-01 -4.60598039e-01  1.67116639e-01\n",
      "   -9.02186724e-02  4.30182283e-01  4.73243710e-01 -1.44160042e-01\n",
      "   -2.97573169e-01 -2.68491319e-02  1.87972586e-01  4.26704729e-02\n",
      "    4.41004808e-01  6.50199329e-01 -8.51804673e-02 -8.41895882e-02\n",
      "    2.45650613e-01 -1.02418207e-01 -2.88872343e-01  1.51858041e-03\n",
      "   -1.35054532e-01  9.05061948e-02 -1.26998124e-01  3.12073091e-01\n",
      "   -2.58306366e-01 -2.91687775e-01 -3.45463293e-01 -4.05822696e-01\n",
      "    1.88966002e-03 -2.21756126e-01 -3.10015101e-01  1.02456363e-01\n",
      "   -2.26054676e-01  4.61669186e-02 -5.12872844e-01 -1.13942433e-01\n",
      "   -1.37108790e-01  4.40179075e-01 -3.47731954e-01 -3.62031235e-01\n",
      "    2.92809306e-01 -2.94301159e-01 -2.26969062e-01 -1.12026354e-01\n",
      "   -4.09533829e-02  8.50277400e-02  8.14443882e-02  2.57699086e-01]\n",
      "  [ 2.00434185e-01 -2.47480148e-01  4.88539497e-01  2.00911838e-01\n",
      "   -2.65707429e-01 -1.61650123e-01 -1.49175831e-01 -4.23540862e-01\n",
      "    3.55740826e-01 -4.01070822e-01 -2.11575818e-01 -1.05493390e-01\n",
      "    4.15648168e-01  2.81708119e-01 -5.34216642e-01  7.90532853e-03\n",
      "    6.53844555e-03 -1.70548749e-01  2.20459558e-01  8.88201684e-03\n",
      "   -2.14107784e-01  7.60057144e-01  6.20929888e-02 -8.09149850e-01\n",
      "   -2.50247497e-01  1.92164443e-01 -1.46661371e-01  1.65210850e-01\n",
      "   -4.96359954e-01  3.68841362e-01  5.19730231e-02  1.41373675e-01\n",
      "    6.82136000e-01  2.45698057e-02 -1.87678329e-01 -3.96129895e-01\n",
      "    2.49225255e-01 -3.29325124e-01  1.97835266e-01  8.53247842e-02\n",
      "    1.08712959e-02 -3.91438639e-01  1.59602855e-01  2.93883492e-01\n",
      "    3.38697800e-01 -8.66721779e-02  6.81244740e-01 -5.39891366e-01\n",
      "   -4.57669460e-01 -3.77134307e-01  1.21210003e-01 -7.47262073e-02\n",
      "    3.21542436e-01 -3.06577290e-01 -2.76220893e-01  2.28041161e-01\n",
      "   -7.82654087e-02  5.04125634e-01  6.70157221e-01 -7.03663127e-01\n",
      "    4.44685561e-01  4.08791198e-01  4.25527228e-02 -1.40357231e-01\n",
      "    2.47800781e-01  4.46827970e-01 -7.05751521e-02 -2.92380744e-01\n",
      "    7.35337286e-01 -5.56341025e-01  9.39835964e-01  3.46314869e-01\n",
      "    2.22692572e-02 -7.80432271e-01  2.18840982e-01 -1.26305578e-01\n",
      "   -2.05067321e-01  3.00121292e-01  3.44566333e-01 -1.07639163e-01\n",
      "   -5.38585730e-02  2.43395799e-01 -1.08524588e-01  3.40714955e-01\n",
      "   -4.07649155e-02  2.17889521e-01 -3.73738888e-01  4.24760006e-02\n",
      "   -1.81097041e-01  4.61844471e-01 -4.24647656e-01 -8.92065305e-02\n",
      "   -1.90528744e-01  4.69083958e-01 -2.19157438e-01  3.35400173e-01\n",
      "    6.24832618e-01  2.22435032e-01 -2.39123834e-01 -9.09668984e-02]\n",
      "  [-1.14506495e+00 -1.11030261e-02  1.34259300e-02 -3.52348629e-01\n",
      "   -1.19198606e+00 -2.60561623e-01  1.43185151e-01 -4.54735197e-01\n",
      "   -4.11121141e-01 -7.20472724e-01  1.80377179e-01  7.42881901e-01\n",
      "    1.17901482e-01  7.83961737e-02  3.44272008e-01  6.75412480e-01\n",
      "    3.86763286e-01  7.50047253e-02 -5.09583182e-01  1.18488622e+00\n",
      "   -4.11095341e-01  2.59906298e-01  4.62356084e-01  3.71483922e-01\n",
      "   -9.97873942e-01 -1.05827239e+00 -2.07779974e-01 -7.63287338e-01\n",
      "   -2.71003557e-01  6.47696182e-02 -4.05356363e-01 -7.07558022e-01\n",
      "   -4.79665159e-01 -5.41621737e-01  4.13114337e-01 -3.04511612e-01\n",
      "    7.07481289e-01  9.15313195e-02  2.84882438e-01  1.38300580e-01\n",
      "    1.45056220e+00  5.74494919e-01  4.60073779e-01 -3.17277319e-01\n",
      "   -4.96595062e-01  2.01986782e-01  7.74518458e-01  7.57064293e-01\n",
      "   -1.46318885e+00  2.54259196e-01 -1.48670799e-01  2.09287972e-01\n",
      "   -5.81125065e-02  7.98536415e-01 -3.21394752e-01 -5.32092561e-01\n",
      "    2.99719928e-02 -3.20798419e-01 -1.41595207e-01 -5.35483711e-01\n",
      "   -6.64361400e-01  1.20227917e+00 -5.46297059e-01  2.05858730e-01\n",
      "    1.92191558e+00  7.91063532e-01 -3.91755636e-01  3.94838716e-01\n",
      "    7.06147844e-02  1.94719896e-01 -2.14356436e-01 -5.70578526e-02\n",
      "    4.99378360e-01 -4.25031116e-01  2.06949555e-01 -3.73850845e-01\n",
      "   -1.33984219e-01  1.06015445e-02 -1.46272028e+00 -8.92012108e-01\n",
      "    5.76074926e-01 -9.74896550e-03 -1.57094453e-01  5.72002060e-01\n",
      "   -7.71049392e-01  3.08992151e-01 -4.80556572e-03  8.10504395e-04\n",
      "    9.13841499e-02  5.40602050e-01 -3.41239224e-01  6.17082274e-01\n",
      "    1.55693082e-01  3.16099121e-01  4.20343023e-01 -2.54488015e-02\n",
      "    2.97785735e-01  5.96978685e-01  4.57309765e-01  2.52244441e-01]\n",
      "  [-5.51776158e-01 -2.64088275e-01 -3.01892302e-02  6.73293745e-01\n",
      "    4.80855125e-01 -5.57028414e-01  2.67175573e-01 -7.51972029e-01\n",
      "    1.30312817e-01  4.19306704e-01 -4.52020608e-01 -3.80144083e-01\n",
      "    1.54740139e-01  3.71601533e-01 -1.27553380e-01 -1.16340786e-01\n",
      "   -1.47605100e+00 -1.48075406e-01  2.28259944e-02  3.85551884e-01\n",
      "   -1.12254845e+00  3.48348774e-01 -6.82779278e-02  4.45629907e-02\n",
      "   -6.52167446e-01  4.74687481e-02  2.00327779e-01 -9.50703811e-01\n",
      "    1.22983583e-01 -1.81549463e-01 -8.63529920e-01 -7.21381420e-01\n",
      "   -4.28857593e-01 -3.40123198e-01 -2.24920981e-01  7.53692068e-02\n",
      "    9.23075517e-01  1.29047741e+00 -5.36219054e-01 -9.42777074e-03\n",
      "   -2.33521596e-01 -3.28907576e-01  6.29490418e-01  2.27659150e-01\n",
      "   -4.21848959e-01 -5.64215060e-01  5.04666320e-01  3.03170329e-01\n",
      "    3.70744915e-01  4.42970584e-01  5.04884469e-01  8.99996196e-01\n",
      "    1.33835500e-01  4.80399221e-01 -3.57648120e-01  7.50947110e-01\n",
      "   -1.07852749e-01  1.04756008e+00  1.82563670e-01 -3.15058804e-01\n",
      "    7.24419752e-01 -5.39413046e-02  2.96979273e-01  8.31131242e-02\n",
      "    1.49646060e+00  6.38363008e-01 -9.00403554e-02  9.17708223e-02\n",
      "    4.89019094e-01  3.49162240e-01 -1.39004335e-01  4.22578551e-01\n",
      "    5.60766867e-01 -2.24225220e-01  1.88438048e-01 -1.34765048e-01\n",
      "    3.32375719e-01  5.24718893e-01 -6.69034819e-01 -9.65393519e-02\n",
      "   -3.06498875e-01 -9.05258234e-03 -1.21171094e+00  9.83699327e-02\n",
      "    4.67505841e-01 -9.01988693e-01 -5.58593115e-01  3.28006007e-01\n",
      "   -1.12765112e+00 -9.11658360e-02 -1.98735841e-01 -7.98530530e-01\n",
      "    2.92542463e-01 -1.50626852e-01 -1.10645111e+00 -1.65304351e+00\n",
      "    5.21007297e-01  7.66851257e-01 -1.26511584e-01 -2.94891473e-02]\n",
      "  [-8.87817870e-02  1.94481752e-01 -1.74527048e-02  3.21096184e-01\n",
      "   -4.00460312e-01 -3.58563082e-01 -1.93359185e-01 -2.29244632e-02\n",
      "    2.53582168e-01  4.70975469e-02 -2.11596635e-01  4.10552453e-02\n",
      "    1.30552328e-01  5.86377039e-02 -8.29724721e-03  8.95011994e-02\n",
      "    7.04519966e-02  1.91633698e-01  7.47369873e-02  3.98774581e-01\n",
      "   -1.76888510e-01  7.15509009e-01  4.71682047e-01  9.33894265e-02\n",
      "   -1.05854025e-01  2.24542202e-02  4.51363827e-01 -1.05802069e-01\n",
      "   -3.03135370e-01 -1.30366907e-01 -1.88804438e-01  1.87963807e-01\n",
      "   -1.67107215e-01 -1.61501604e-01 -3.91298640e-02 -2.53160449e-01\n",
      "    1.21918911e-01  1.07714061e-01 -1.18568831e-01 -1.23044798e-01\n",
      "   -5.41583784e-03 -1.31622465e-01  4.97502235e-02  2.68038283e-02\n",
      "    1.50242192e-01  2.18954101e-01  3.71432206e-01 -1.68336576e-01\n",
      "   -3.31457327e-01 -1.16257181e-02  1.97992461e-02 -1.61043848e-01\n",
      "    1.06674012e-01  2.45531030e-01 -6.67856846e-02  3.18805496e-01\n",
      "    4.91986105e-02  3.06727786e-01  3.74958374e-01  1.60353366e-01\n",
      "    2.93700267e-02  1.03999597e-01  2.60365437e-01 -3.35917937e-01\n",
      "   -2.67271124e-02  3.67473512e-02  1.81767897e-01  7.55860101e-02\n",
      "    5.80708435e-01 -1.34771227e-01 -1.25592186e-02 -1.42222246e-02\n",
      "    2.68079162e-01  1.72276419e-01  2.69600093e-01  2.64435254e-01\n",
      "    2.98425197e-01 -5.86652074e-02  2.43624313e-01 -1.07880558e-01\n",
      "    4.48194656e-01  1.61529525e-01 -3.20382454e-01  4.40086264e-02\n",
      "   -2.11100379e-01  1.08935142e-01  1.88499386e-01  2.81892524e-01\n",
      "    4.95311848e-02  4.38847894e-01 -1.48532722e-01 -1.59399531e-01\n",
      "   -8.01013668e-02 -1.18038834e-01 -2.08997294e-01 -1.18928851e-01\n",
      "    3.03973740e-01  2.73169120e-01 -2.13729089e-01  7.23406163e-02]\n",
      "  [ 2.44634318e-02  5.63438056e-01  5.89344972e-01  1.29458445e+00\n",
      "   -8.46549368e-01  2.20069468e-01 -3.18755006e-01  2.84541430e-01\n",
      "    3.38557848e-01 -6.89593240e-02  6.85415673e-01  2.22148660e-01\n",
      "   -1.11808546e+00 -1.48621678e-01  1.47308594e-02  6.96487402e-02\n",
      "   -6.56856136e-01 -7.76547320e-01  3.44906121e-01 -1.21335270e-01\n",
      "    2.64077700e-01  8.67389663e-01  4.39711268e-01  7.40579210e-01\n",
      "   -1.80980667e-01 -4.17958164e-01 -2.68391843e-01  2.51543065e-01\n",
      "    8.53563345e-01 -1.95440712e-01  8.59338737e-01 -1.71613881e-01\n",
      "   -7.68284027e-01 -8.39479450e-02 -1.11868731e+00 -5.99705384e-01\n",
      "    3.94721804e-01  2.61985718e-01 -1.22795633e-01  3.68964660e-01\n",
      "    4.74037492e-01 -6.73518565e-01  2.40599574e-01 -8.50136528e-01\n",
      "   -2.69429306e-02 -2.31124071e-01 -3.03934525e-01  1.11751259e-01\n",
      "   -2.52280703e-01  1.06298458e+00  3.86991732e-01 -5.26380664e-01\n",
      "   -4.07302363e-01 -2.58749069e-02  2.01282635e-02 -3.42501823e-01\n",
      "   -7.55092253e-01  3.51994961e-01  2.21654713e-01  9.46028045e-01\n",
      "   -2.46403727e-01  7.08726809e-01  9.35901315e-01  2.51090085e-01\n",
      "   -3.21680458e-02  1.02898299e+00  5.10891410e-01 -3.46075491e-02\n",
      "    7.12450771e-02 -2.19969912e-01 -4.97936472e-01 -4.77229734e-01\n",
      "   -2.75711023e-01 -3.12760263e-01  4.19100025e-01 -5.18605074e-03\n",
      "    2.97501078e-01  7.71372440e-02 -4.85107216e-01 -8.95391700e-02\n",
      "   -3.36790277e-01 -3.98183039e-01  6.19358992e-01  2.14214883e-02\n",
      "   -2.38401433e-01 -2.64436227e-01  7.86162704e-01  3.18056640e-01\n",
      "    2.86156281e-01 -2.67366834e-01  1.59762789e-01 -5.29544323e-01\n",
      "    4.53323453e-01 -4.07092858e-01 -3.71390119e-01 -6.43415194e-01\n",
      "    3.70459690e-02  7.50385305e-01 -3.98078502e-01  7.95193691e-02]\n",
      "  [-3.59489027e-01  1.33082822e-01  2.14496990e-02  3.09113689e-01\n",
      "   -1.48068220e-01 -7.13028086e-02 -4.50163548e-01 -3.34106892e-01\n",
      "    3.44627873e-01  1.36215032e-01 -2.85374947e-01 -1.07367372e-01\n",
      "    7.81449053e-02 -8.29977054e-02 -4.12700230e-01 -1.97200193e-01\n",
      "   -3.29293287e-02  2.78826945e-01  1.17068541e-01 -1.23887480e-01\n",
      "    9.41594027e-02  4.71760439e-01  2.72793386e-01  1.00843496e-01\n",
      "   -2.69506489e-01  6.48677647e-02  3.85506978e-01  9.81747157e-02\n",
      "   -8.64174237e-02  1.32362890e-02  2.19941057e-01  2.04082948e-01\n",
      "    3.35436838e-01 -3.23447388e-01 -6.52018810e-02 -2.64076483e-03\n",
      "   -3.95950429e-02  1.66849260e-01  1.47308263e-01 -9.68357051e-02\n",
      "   -1.69340335e-01 -4.50206810e-03  4.45423489e-02 -3.47097170e-01\n",
      "    3.36732607e-02 -1.23852125e-01  8.91359429e-02 -4.79478214e-01\n",
      "   -3.16519834e-02  5.76669778e-02 -8.44174991e-02 -4.01740097e-02\n",
      "   -2.06617042e-01  2.93336346e-02 -1.42148692e-01 -1.99777324e-01\n",
      "   -2.38544269e-01  5.08229128e-01  4.17670796e-01  2.61508645e-01\n",
      "    2.89054191e-01  1.39308726e-01  4.75137583e-01 -3.58501471e-01\n",
      "    5.00338186e-02 -2.63109556e-02  5.53162852e-02 -1.20673325e-01\n",
      "    3.02929791e-01 -3.64299835e-01  9.74748748e-02  6.01063622e-02\n",
      "   -5.33307348e-04  3.51673739e-02  2.10547009e-01 -1.82351604e-01\n",
      "    2.31844233e-01 -1.10495717e-01  2.81664261e-01 -2.56971973e-01\n",
      "    3.96006779e-01  2.83910107e-01 -1.37558483e-01  2.37945366e-02\n",
      "   -7.22366625e-02  2.94926863e-01 -3.40629257e-01 -4.06837334e-02\n",
      "   -2.44196289e-02  4.77534157e-01 -3.20376315e-01  3.66754457e-01\n",
      "   -2.11008040e-01  8.52695784e-02  1.13754982e-01  9.61713195e-04\n",
      "    2.67885036e-01  1.49757237e-01 -2.61354555e-01 -7.26055223e-02]\n",
      "  [-8.55568900e-02  1.20332189e-01 -1.02467064e-01  1.28768304e-01\n",
      "   -4.62358895e-01 -4.99297847e-01 -7.43007572e-02  6.22963684e-02\n",
      "   -1.57117585e-01  5.51112214e-01  1.72492797e-02  1.41660502e-01\n",
      "   -4.76286233e-02 -3.09552837e-01  1.92320355e-01 -1.57321010e-01\n",
      "   -2.02272952e-01  9.25061411e-02 -1.03652236e-01  3.55272065e-01\n",
      "   -1.16799383e-01  6.21862854e-02  2.25165014e-01  4.71433234e-01\n",
      "   -2.95378581e-01 -3.05120131e-01  6.74290229e-01 -3.90725176e-01\n",
      "   -2.54076024e-02 -2.40087560e-01  7.63907944e-03  3.35243986e-01\n",
      "    2.25982678e-02 -2.51139155e-01  1.17844564e-01 -2.82850834e-01\n",
      "   -7.31152224e-02 -4.19043549e-02 -4.07608268e-02 -4.14920330e-01\n",
      "   -5.32537697e-02 -2.95942340e-01 -1.81735447e-01  3.16153903e-02\n",
      "    4.57407933e-01  1.20093416e-01 -1.17292472e-01 -1.76640825e-01\n",
      "   -2.93028478e-01  2.60679956e-01  3.10812881e-01 -1.35782092e-02\n",
      "    3.83985705e-01  1.68701890e-01 -9.82333750e-02  2.71657246e-01\n",
      "   -2.55981070e-01  2.72022049e-01  6.06073671e-01 -5.93752407e-02\n",
      "   -3.67493398e-03 -1.09204009e-01  4.24198549e-01  6.02918299e-02\n",
      "    3.53401440e-01  3.09896887e-01 -2.08463395e-01 -2.69147010e-01\n",
      "    2.91675359e-01  6.65988095e-02 -7.03321421e-01  1.45963553e-02\n",
      "    3.46742304e-02  4.17559597e-01 -1.37864443e-01  3.60843891e-01\n",
      "    1.67992774e-03 -2.80380785e-01 -2.14979681e-01 -4.70442832e-01\n",
      "    4.45949308e-01 -9.39986137e-02 -4.27007171e-01  2.45060569e-01\n",
      "   -2.00309220e-01  8.56871450e-02 -2.02995906e-01 -1.38350071e-02\n",
      "    1.61750553e-01  4.07733077e-01 -3.41608380e-01 -3.13931955e-01\n",
      "    4.20605433e-02 -5.97900455e-01 -2.96732038e-01 -3.16172248e-01\n",
      "    9.33172395e-02  7.56283631e-02 -4.85215523e-02  2.49002347e-01]\n",
      "  [ 2.37196230e-02  6.69139704e-02  3.11567626e-01  1.93946394e-01\n",
      "   -2.42632719e-01 -1.30734379e-01 -2.84668003e-01 -2.73689641e-01\n",
      "    2.82369272e-01  1.89606713e-01 -2.07625397e-01  1.04197028e-01\n",
      "    3.04270604e-01  1.72203695e-01  9.99882596e-02  1.93277651e-01\n",
      "    1.36727112e-01  1.31577192e-01  2.44676715e-01  2.57295403e-01\n",
      "   -2.03416643e-01  3.71507421e-01  4.07141291e-01  3.93311812e-01\n",
      "   -2.08414152e-01 -6.93863419e-02  4.64405131e-01 -2.25200899e-01\n",
      "   -9.15477919e-02 -2.03825953e-01 -3.87403541e-01  1.93534704e-01\n",
      "   -3.24383234e-01 -2.57988005e-01 -3.31914116e-01 -1.09111091e-01\n",
      "   -9.37361411e-03 -8.25657992e-02  8.04302060e-02 -3.90623673e-01\n",
      "   -1.69413515e-01 -1.69937571e-03 -2.83261518e-01 -3.17763203e-01\n",
      "   -1.86438596e-02  2.52560167e-01  2.68905385e-01 -4.95352947e-01\n",
      "    3.42496908e-03  1.01692768e-01 -9.59922933e-02 -3.39711009e-01\n",
      "   -9.84034498e-02  1.13895456e-01  5.95173710e-02  4.57811936e-01\n",
      "   -2.99195677e-01  4.49312895e-01  3.72349465e-01  4.34062951e-01\n",
      "    1.03922512e-01  3.71126630e-02  2.35889666e-01 -1.57109265e-01\n",
      "   -4.02297963e-02 -1.15896987e-01  9.90787228e-02 -1.31970032e-01\n",
      "    4.44726887e-02  2.57253366e-01 -8.38368151e-03 -4.03739829e-01\n",
      "   -1.96293507e-01  2.18713483e-01  2.13422244e-02 -1.14203110e-01\n",
      "    3.53793405e-01 -2.82495573e-01  3.80945196e-01 -5.15160554e-02\n",
      "    4.21388345e-01  9.06143303e-02 -3.59432984e-01  6.45996122e-02\n",
      "   -2.00384258e-01  7.41236362e-02  1.03572869e-01 -1.99327369e-01\n",
      "    2.29436720e-01  3.98746980e-01 -2.49220868e-01 -8.07215503e-04\n",
      "   -4.30391790e-01 -5.04080268e-02 -2.75751064e-01  1.08414156e-01\n",
      "    9.58137464e-02  2.72362040e-01 -1.21411822e-01  4.20190256e-02]\n",
      "  [-7.58255921e-01  7.43771913e-01 -5.35518494e-01  2.11012910e-01\n",
      "   -1.65406892e-02  2.93474367e-01  1.27429515e-01 -5.61768491e-02\n",
      "    2.68214102e-01  1.35552517e-01 -4.54509625e-01  7.65691170e-02\n",
      "    3.74128495e-01  7.16447522e-01  7.71596098e-02  8.43460634e-03\n",
      "    5.27289902e-01  2.80363945e-01  2.07284782e-01 -1.51031270e-01\n",
      "   -8.14024756e-01  5.08827327e-01 -2.76994516e-02  7.71715056e-02\n",
      "   -5.18355786e-02 -2.66884350e-01  1.49624949e+00 -1.10841377e+00\n",
      "   -7.09744595e-02  7.96734812e-01 -2.98296961e-01  8.31113283e-01\n",
      "   -1.50502861e-01 -1.11521997e+00  7.92683191e-01 -1.98340520e-01\n",
      "   -1.16317328e-01  4.98666755e-01 -8.52701494e-01 -3.55726099e-01\n",
      "   -6.11058303e-01 -5.79050587e-01 -8.33480603e-01 -4.83956797e-01\n",
      "   -6.27022781e-01  1.40087394e+00  3.18774039e-01  7.49836895e-01\n",
      "   -6.83035163e-02  1.26848699e-01 -3.37920366e-01  3.41090374e-01\n",
      "    7.85091590e-01 -3.62430432e-01  8.48497022e-02  2.88470359e-01\n",
      "   -9.79708033e-01  2.98556873e-01 -9.16640423e-01  7.59806435e-01\n",
      "   -5.41260824e-01  1.64018421e-01 -4.33716820e-01  2.54649620e-01\n",
      "    7.82807226e-01 -4.12151456e-01 -6.18520365e-02 -2.74644151e-01\n",
      "   -1.80722074e-01 -1.02013661e+00 -2.44817257e-01 -8.75485367e-01\n",
      "   -2.28001522e-01  5.04544932e-02  5.87878518e-01  2.13801200e-01\n",
      "    1.44954392e-01 -1.33869182e-01  5.40035733e-01 -1.21840961e+00\n",
      "    5.77712381e-01  2.24632367e-01 -9.79130368e-01 -3.96181939e-01\n",
      "   -4.81691790e-01  2.94758439e-01 -1.12417443e+00  4.36899423e-01\n",
      "   -9.12498864e-02  7.13559023e-01 -6.97902483e-01  6.10890484e-01\n",
      "   -7.74982310e-01 -8.48837122e-02 -1.09734843e-01 -1.57084665e-01\n",
      "    5.41383957e-01  4.01766087e-01  1.77314599e-01  1.07130992e+00]\n",
      "  [ 2.32617808e-01  5.75820562e-02 -1.41679220e-01  6.13003677e-01\n",
      "   -5.99224111e-01 -4.39621255e-01 -3.58203721e-01  2.64658435e-01\n",
      "    7.07095376e-02 -5.47206943e-01 -5.35801257e-02 -7.80055088e-02\n",
      "   -6.57129507e-01 -1.30220044e-01 -1.59571411e-02  6.03504141e-01\n",
      "    2.36347739e-01 -1.96707730e-01  5.78216690e-01  3.53598960e-01\n",
      "    5.49518455e-02  3.89915632e-01 -2.34155019e-02  5.92671850e-01\n",
      "   -4.96821808e-01  7.37699984e-01 -1.80603116e-01 -7.52240712e-01\n",
      "    1.11663131e-01 -1.03797973e+00 -2.81474737e-01  4.44128068e-02\n",
      "    6.84362239e-01 -2.30291203e-01 -4.11082204e-01 -4.10213501e-01\n",
      "    6.05893695e-01 -2.55303448e-01  5.35638492e-02 -1.67803359e-02\n",
      "    6.10423032e-01  1.12700959e-01 -6.99526979e-02  4.20926205e-01\n",
      "   -1.50121854e-01 -2.68829379e-01  3.18185727e-01  3.62546471e-01\n",
      "   -2.92337017e-01 -1.26639540e-01  6.11711815e-01  1.99518881e-02\n",
      "    2.38534296e-01 -5.00502800e-01 -1.97956776e-01  2.38562919e-01\n",
      "   -2.32236718e-03 -1.70966223e-02  7.73550166e-01 -7.34225326e-02\n",
      "   -4.23476021e-01  1.46825329e-01  1.01192017e-01 -1.66149684e-01\n",
      "    3.67700995e-01 -2.23446918e-01  1.84784332e-01 -1.96843799e-01\n",
      "   -3.98631790e-01 -5.49836168e-01 -2.75805074e-01 -9.11679361e-01\n",
      "   -1.19517407e-01  1.43632407e-01  2.17309251e-01 -9.30502411e-02\n",
      "    2.88454812e-01  1.86396411e-01 -1.52732870e-01  2.27398896e-01\n",
      "    4.74492533e-01  8.97552362e-02 -5.36057027e-01  1.71365752e-01\n",
      "    8.93420054e-02  6.77932285e-01  4.46676421e-01  5.57023654e-02\n",
      "    1.55734019e-01 -2.80440405e-01 -8.44911218e-02  3.64448471e-01\n",
      "    8.67004388e-02  1.44373464e-01 -5.67775271e-01  3.86962656e-01\n",
      "    4.94069315e-01 -2.27699256e-01  6.33111275e-01 -9.11024140e-02]\n",
      "  [ 1.06503154e-02 -1.68172737e-01  1.45875944e-01 -4.26543686e-01\n",
      "    9.38263051e-02  3.10722486e-02  7.92843321e-02 -7.22788901e-02\n",
      "    3.36819968e-02  1.00643686e-01  7.64289792e-02  2.44329603e-01\n",
      "    1.67288299e-01 -1.36717168e-01 -1.11060614e-03  1.44801425e-02\n",
      "   -2.38701068e-02  2.87646969e-02 -2.11745777e-01  2.63109969e-02\n",
      "   -4.31055734e-02 -1.45467075e-01  2.34268682e-01  9.82367225e-03\n",
      "   -1.87021388e-01 -4.76750032e-01 -3.13344152e-01 -9.36612909e-02\n",
      "   -1.96897049e-01  2.60798224e-01  1.85929264e-01  7.74648814e-02\n",
      "   -4.34109250e-01  2.64049241e-02 -1.01654643e-01 -1.97562487e-01\n",
      "   -3.02058507e-02  1.70587969e-01  3.32894099e-02  1.44077297e-01\n",
      "   -5.68563061e-02  4.42719281e-02 -3.76025358e-02 -5.48266012e-02\n",
      "    8.06804286e-02  1.02662809e-01  2.21365687e-01 -1.51428315e-01\n",
      "   -1.76061119e-01 -6.09672238e-02 -2.29565647e-01  2.74383288e-02\n",
      "   -3.20774335e-02  1.67766718e-01  1.92913845e-01 -1.32703334e-01\n",
      "    6.60410577e-02 -4.96567950e-02 -3.22632129e-01  4.55280113e-02\n",
      "    1.53556757e-01  3.14631442e-01  2.29746203e-01  3.72043251e-02\n",
      "    5.54644096e-02  2.01664894e-01 -2.01412454e-01  1.06104617e-01\n",
      "    7.96009337e-02  5.42402865e-02  3.63483325e-02  2.01927493e-01\n",
      "    2.73798415e-02 -3.98820877e-01 -1.76731038e-01 -1.56914522e-01\n",
      "   -2.64966949e-03  3.63461176e-01 -3.63986836e-02 -1.33451332e-01\n",
      "   -7.80343621e-02  8.05584690e-02 -1.74198731e-01 -8.75758950e-02\n",
      "   -2.96643967e-01 -1.17527935e-01  1.60854217e-01 -8.91688094e-02\n",
      "   -2.50228913e-03 -1.50123745e-02 -2.36176232e-02 -1.63806548e-01\n",
      "    1.96448735e-01 -2.36804828e-01  1.26901191e-02  5.78573871e-02\n",
      "    1.35559308e-01  1.95217288e-01 -3.14434957e-01  1.37190067e-01]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]]\n",
      "['in', 'honor', 'of', 'an', 'upcoming', 'trip', 'to', 'vegas', 'i', 'thought', \"i'd\", 'review', 'some', 'of', 'my', 'most', 'and', 'least', 'favorite', 'places', 'in', 'the', 'town']\n",
      "in\n",
      "of\n",
      "an\n",
      "trip\n",
      "to\n",
      "vegas\n",
      "i\n",
      "thought\n",
      "review\n",
      "some\n",
      "of\n",
      "my\n",
      "most\n",
      "and\n",
      "least\n",
      "favorite\n",
      "places\n",
      "in\n",
      "the\n",
      "town\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Returns\n",
    "-------\n",
    "X: array\n",
    "    Array of dimensions (SENTENCES_SIZE, MAX_SENTENCE_LENGTH, EMBEDDING_DIM) where \n",
    "    the first dimension denotes the index of the sentence in the dataset and second is \n",
    "    the word index in the sentence. Sentences that are shorter than MAX_SENTENCE_LENGTH\n",
    "    are padded with zero vectors. Words that are not in the vocabulary are also \n",
    "    represented with zero vectors of EMBEDDING_DIM size.\n",
    "S: array\n",
    "    Array of SENTENCES_SIZE dimension containing the sentence lenghts\n",
    "\"\"\"\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "X = np.zeros((SENTENCES_SIZE,MAX_SENTENCE_LENGTH,EMBEDDING_DIM))\n",
    "S = np.zeros(SENTENCES_SIZE)\n",
    "\n",
    "for sent_pos , sent in enumerate(all_sentences):\n",
    "    S[sent_pos] = len(sent)\n",
    "    for word_pos , word in enumerate(sent):\n",
    "        if word in word_to_ind:\n",
    "            X[sent_pos,word_pos] = emb_matrix[word_to_ind[word]]\n",
    "        else:\n",
    "            X[sent_pos,word_pos] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "#some quick testing:\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "print(X[:1])\n",
    "print(all_sentences[0])\n",
    "for word in all_sentences[0]:\n",
    "    if word in word_to_ind:\n",
    "        print(word)\n",
    "np.set_printoptions(threshold=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train on a subset of data, and test on remaining data. Your task is to split X, Y and S into training and test set (60%-40%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns\n",
    "-------\n",
    "X_train, y_train, s_train: arrays\n",
    "    Randomly selected 60% of all data\n",
    "X_test, y_test, s_test: arrays\n",
    "    Rest of the data\n",
    "\"\"\"\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "len_x_train  = int(0.6 * len(all_sentences))\n",
    "len_x_test   = len(all_sentences) -len_x_train\n",
    "\n",
    "X_train_mask = np.random.choice(X.shape[0], size=len_x_train, replace=False)\n",
    "X_test_mask = np.random.choice(X.shape[0], size=len_x_test, replace=False)\n",
    "\n",
    "X_train = X[X_train_mask]\n",
    "X_test = X[X_test_mask]\n",
    "y_train = Y[X_train_mask]\n",
    "y_test = Y[X_test_mask]\n",
    "s_train = S[X_train_mask]\n",
    "s_test = S[X_test_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM implementation in tensorflow. Inputs are padded sequences of word vectors, sentence lengths, and true labels (0 or 1). The model takes word vectors and passes them through the LSTM. Final state is used as an input of one fully connected layer with output dimension 1. We also get probability that the class is positive and argmax label. Network uses Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, cell_dim=64):\n",
    "        \"\"\"\n",
    "        Attributes\n",
    "        ----------\n",
    "        x: float\n",
    "            Input sentence of shape (BATCH SIZE, MAX SENTENCE LENGTH, EMBEDDING DIM)\n",
    "        y: float\n",
    "            Output label of shape (BATCH SIZE)\n",
    "        s: float\n",
    "            Length of sentences of shape (BATCH SIZE)\n",
    "        last_state: float\n",
    "            The last state of sequences with shape (BATCH SIZE, CELL DIM)\n",
    "        logits: float\n",
    "            The \n",
    "        prob: float\n",
    "            Probabilities after sigmoid\n",
    "        y_hat: int\n",
    "            Predicted class value (0 or 1)\n",
    "        loss: float\n",
    "            Cross entropy loss\n",
    "        optimize:\n",
    "            Operation that updates the weights based on the loss\n",
    "        accuracy: float\n",
    "            Accuracy of prediction y_hat given y\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "          ### YOUR CODE HERE ### \n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, [None, None, EMBEDDING_DIM])\n",
    "        self.y = tf.placeholder(tf.float32, [None])\n",
    "        self.s = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "\n",
    "        \"\"\" \n",
    "        Use dynamic_rnn to define an LSTM layer\n",
    "        Define last_state as class attribute to be the last output h of LSTM\n",
    "        (Note that we have zero padding)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ### \n",
    "        cell = tf.keras.layers.LSTMCell(cell_dim)\n",
    "        _, states = tf.nn.dynamic_rnn(cell, self.x, sequence_length=self.s, dtype=tf.float32)\n",
    "\n",
    "        self.last_state = states[1]\n",
    "        \n",
    "        \"\"\"\n",
    "        Define logits, prob and y_hat as class attributes. \n",
    "        We get logits by applying a single dense layer on the last state.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ### \n",
    "        self.logits = tf.squeeze(tf.layers.dense(self.last_state, 1), -1)\n",
    "        self.prob = tf.sigmoid(self.logits)\n",
    "        self.y_hat = tf.cast(self.prob > 0.5, tf.float32)\n",
    "\n",
    "        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.y, logits=self.logits))\n",
    "        self.optimize = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we finally train our RNN model and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f51794e7f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f51794e7f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f51794e7f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f51794e7f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Iter: 100 Train loss: 0.59491736\n",
      "Iter: 200 Train loss: 0.4079355\n",
      "Iter: 300 Train loss: 0.36749715\n",
      "Test loss: 0.5449981 Test accuracy: 0.7441666666666666\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "model = LSTM()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for iter in range(300):\n",
    "        i = np.random.randint(0, X_train.shape[0], 64)\n",
    "        feed = { model.x: X_train[i], model.y: y_train[i], model.s: s_train[i] }\n",
    "        _ = sess.run(model.optimize, feed)\n",
    "        \n",
    "        if (iter + 1) % 100 == 0:\n",
    "            train_loss = sess.run(model.loss, feed)\n",
    "            print('Iter:', iter + 1, 'Train loss:', train_loss)\n",
    "\n",
    "    test_loss, test_pred = sess.run([model.loss, model.y_hat], { model.x: X_test, model.y: y_test, model.s: s_test })\n",
    "    print('Test loss:', test_loss, 'Test accuracy:', np.mean(test_pred == y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-ML-py",
   "language": "python",
   "display_name": "Python [conda env:ML] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}