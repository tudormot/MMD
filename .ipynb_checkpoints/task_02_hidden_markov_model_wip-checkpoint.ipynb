{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project task 02: Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "from scipy.sparse import linalg as spl\n",
    "from collections import defaultdict\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "In this proejct task, we use hidden markov models (HMMs) as a probabilistic generative model for text data. Intuitively, we can think of each latent variable $Z_t \\in \\lbrace 1,...,K \\rbrace$ as, e.g., a hidden part-of-speech tag (like noun, verb, or adjective). Each observation $X_t\\in \\lbrace 1,...,V \\rbrace$ is a word. $K$ denotes number of possible states and $V$ denotes the number of words in our vocabulary. The model generates a sequence of words (i.e. a sentence) as follows:\n",
    "* Generate $Z_1$ from the initial probability distribution ${\\pi} \\in \\mathbb{R}^K$:\n",
    "\\begin{equation}\n",
    "Pr(Z_1=k) = \\mathbf{\\pi}_k.\n",
    "\\end{equation}\n",
    "* Given $Z_1$, generate $Z_2,Z_3,...,Z_t$ as :\n",
    "\\begin{equation}\n",
    "Pr(Z_{t+1}=j|Z_t=i) = \\mathbf{A}_{ij},\n",
    "\\end{equation}\n",
    "where $\\mathbf{A} \\in \\mathbb{R}^{K\\times K}$ is the state transition probability matrix.\n",
    "\n",
    "* Given $Z_t$, generate $X_t$ as :\n",
    "\\begin{equation}\n",
    "Pr(X_{t}=v|Z_t=i) = \\mathbf{B}_{iv},\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Indeed, given a parameter set $\\lbrace \\mathbf{A},\\mathbf{B},\\pi \\rbrace$ we can generate a set of sentences. Of course the set can contain sentences of different sizes.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to classify a given sentence as either 1-star or 5-star. For this task, roughly we consider the following procedure:\n",
    "* We select a subset of 1-star (resp. 5-star) reviews as training set. (will be stored in variables `reviews_1star_train` (resp. `reviews_5star_train`))  \n",
    "* We learn **two** HMMs: HMM $\\mathcal{H}_1$ on `reviews_1star_train` and HMM $\\mathcal{H}_5$ on `reviews_5star_train`.\n",
    "* In test phase, we classify a sentence based on the likelihood of the sentence in $\\mathcal{H}_1$ and $\\mathcal{H}_5$ and class probability of 1-star and 5-star reviews.\n",
    "\n",
    "The above steps are elaborated upon in Section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with a subset of reviews for restaurants in Las Vegas. The reviews that we'll be working with are either 1-star or 5-star. \n",
    "You can download the used data set (`task03_data.npy`) from:\n",
    "\n",
    "* ([download link](https://syncandshare.lrz.de/dl/fi7cjApuE3Bd3xyfsyx3k9jr/task03_data.npy)) the preprocessed set of 1-star and 5-star reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"task03_data.npy\")\n",
    "reviews_1star = data.item()[\"reviews_1star\"]\n",
    "reviews_5star = data.item()[\"reviews_5star\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `reviews_1star` (resp. `reviews_5star`) is a list of sentences of 1-star (resp. 5-star) reviews. Each sentence is itself a list of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect how the sentences look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1-star review: ['This', 'place', 'tops', 'the', 'least', 'favorite', 'list', 'by', 'a', 'long', 'shot']\n",
      "a 5-star review: ['Filet', 'mignon', 'and', 'lobster', 'tail', 'was', 'very', 'good']\n"
     ]
    }
   ],
   "source": [
    "print(\"a 1-star review: \" + str(reviews_1star[1]))\n",
    "print(\"a 5-star review: \" + str(reviews_5star[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementaion\n",
    "\n",
    "A set of sentences are assumed to be generated from an HMM $\\mathcal{H}$ as explained in introduction.\n",
    "We denote parameters of $\\mathcal{H}$ by $\\lbrace \\mathbf{A},\\mathbf{B},\\pi \\rbrace$. Given the set of sentences, your task is to learn the unknown parameters $\\lbrace \\mathbf{A},\\mathbf{B},\\pi \\rbrace$ via EM algorithm for HMMs, i.e.,  Baum-Welch algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a helper class `HMM_Params` that contains three randomly-initialized stochastic matrices $\\mathbf{A}\\in \\mathbb{R}^{K\\times K}$, $\\mathbf{B}\\in \\mathbb{R}^{K\\times V}$, and $\\pi \\in \\mathbb{R}^{K\\times 1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM_Params:\n",
    "    \n",
    "    def __init__(self,n_states,n_symbols):\n",
    "        \"\"\" Makes three randomly initialized stochastic matrices `self.A`, `self.B`, `self.pi`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_states: int\n",
    "                  number of possible values for Z_t.\n",
    "        n_symbols: int\n",
    "                  number of possible values for X_t.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        self.A  = self.rnd_stochastic_mat(n_states,n_states)\n",
    "        self.B  = self.rnd_stochastic_mat(n_states,n_symbols)\n",
    "        self.pi = self.rnd_stochastic_mat(1,n_states).transpose()\n",
    "        \n",
    "        \n",
    "    def rnd_stochastic_mat(self,I,J):\n",
    "        \"\"\" Retruns a randomly initialized stochastic matrix with shape (I,J).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        I: int\n",
    "           shape[0] of desired matrix.\n",
    "        J: int\n",
    "           shape[1] of disired matrix.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        x: np.ndarray\n",
    "           a rondom stochastic matrix with shape (I,J)\n",
    "        \n",
    "        \"\"\"\n",
    "        x = np.full((I,J),(1/J))\n",
    "        x = x + (np.random.randn(I,J)*(1.0/(J*J)))\n",
    "        x = x/np.reshape(np.sum(x,axis=1),newshape=(I,1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a helper class `HMM_TxtGenerator`.\n",
    "Complete the implementation of the following functions:\n",
    "* `HMM_TxtGenerator.forwards_backwards(sentence_in)`\n",
    "* `HMM_TxtGenerator.E_step(sentence_in)`\n",
    "* `HMM_TxtGenerator.generate_sentence(sentence_length)`\n",
    "\n",
    "For parameter describtions, please refer to the headers below. \n",
    "\n",
    "Note that `HMM_TxtGenerator` is a model which will be defined only on one curpos. We are going to make two instances of this class, one for 1-star reviews and one for 5-star reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM_TxtGenerator:\n",
    "    def __init__(self,corpus,K):\n",
    "        \"\"\"Given the set of sentences `corpus` and number of states `K`, builds an HMM.\n",
    "           Firstly it makes the volcabulary `self.word_list` based on all present words in \n",
    "           `corpus`. The variable `self.word_list` is a list of words. Then index of the word\n",
    "           `self.word_list[v]` is v. Moreover, this function constructs `self.model_params`\n",
    "           which is an instance of randomly initialized `HMM_Params`.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : A list of sentences. Each sentence is a list of words.  \n",
    "            We will learn model_params using sentences in `corpus`.\n",
    "        K: int\n",
    "           Number of possible states, i.e. Z_t \\in {0,...,K-1}\n",
    "        \n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        None :\n",
    "        \"\"\"\n",
    "        self.corpus = corpus.copy()\n",
    "        self.K = K\n",
    "        #collect all words ---\n",
    "        word_dic = {}\n",
    "        for sent in self.corpus:\n",
    "            for w in sent:\n",
    "                if(w in word_dic):\n",
    "                    word_dic[w] = word_dic[w] + 1\n",
    "                else:\n",
    "                    word_dic[w] = 1\n",
    "        self.word_list = [u for u in word_dic.keys()]\n",
    "        self.word_dic  = word_dic\n",
    "        self.V = len(self.word_list)\n",
    "        #init params\n",
    "        self.model_params = HMM_Params(K,len(self.word_list))\n",
    "    \n",
    "    def forwards_backwards(self,sentence_in):\n",
    "        \"\"\"Does the forwards-backwards algorithm for an observed list of words\n",
    "           (i.e. and observed sentence).\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        sentence_in : a list of T words. Each word is a string.\n",
    "                      You can convert `sentence_in` to a sequence of word-indices\n",
    "                      as `x = self.sentence_to_X(sentence_in)`. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        alpha : np.ndarray, shape=(T,K)\n",
    "                alpha(t,k) = Pr(Z_t=k,x[1:t])\n",
    "        beta  : np.ndarray, shape=(T,K)\n",
    "                beta(t,k)  = Pr(X_{t+1:T}|Z_t=k)\n",
    "        log_likelihood  : scalar\n",
    "                log probability of evidence, Pr(X_{1:T}=sentence_in) \n",
    "        \"\"\"\n",
    "#         T = len(sentence_in)\n",
    "#         x = self.sentence_to_X(sentence_in)\n",
    "\n",
    "#         #compute de forward pass\n",
    "#         alpha = np.zeros((T,self.K))\n",
    "#         beta  = np.zeros((T,self.K))\n",
    "        \n",
    "#         #print('debug. shapes of pi = %s , B[:,x[0]] = %s, alpha[0] = %s'%(str(self.model_params.pi.shape),str(self.model_params.B[:,x[0]].shape), str(alpha[0])))\n",
    "#         alpha[0] = self.model_params.pi.flatten()*self.model_params.B[:,x[0]]\n",
    "        \n",
    "#         for t in range (1,T):\n",
    "#             alpha[t] = np.multiply(self.model_params.B[:,x[t]],self.model_params.A.transpose()@alpha[t-1])\n",
    "        \n",
    "#         #backwards pass:\n",
    "        \n",
    "#         beta[-1] = 1\n",
    "#         for t in range(T-2,-1,-1):\n",
    "#             beta[t] = self.model_params.A @ np.multiply(self.model_params.B[:,x[t+1]],beta[t+1])\n",
    "        \n",
    "#         #print(\"debuggy. this is where the log 0 happens.\")\n",
    "#         #print(\"alpha[-1] is %s\"%str(alpha[-1]))\n",
    "#         log_likelihood = np.log(np.sum(alpha[-1]))\n",
    "#         #print('end debuggy')\n",
    "        \n",
    "#         return alpha,beta,log_likelihood\n",
    "        \n",
    "  #get A,B,pi ------ \n",
    "        A  = self.model_params.A  #[KxK]\n",
    "        B  = self.model_params.B  #[KxV]\n",
    "        pi = self.model_params.pi #[Kx1] ------\n",
    "        #convert sentence to numbers \n",
    "        x = self.sentence_to_X(sentence_in)\n",
    "        #make alpha\n",
    "        T = len(sentence_in)\n",
    "        K = A.shape[0]\n",
    "        alpha = np.zeros([T,K])\n",
    "        alpha[0,:] = np.reshape(pi , newshape=(K)) *\\\n",
    "                     np.reshape(B[:,x[0]],newshape=(K))\n",
    "        A_transpose = np.transpose(A)\n",
    "        for t in range(T-1):\n",
    "            alpha[t+1,:] = np.reshape(B[:,x[t+1]] *\\\n",
    "                           (np.dot(A_transpose,alpha[t,:])), newshape=(K)) \n",
    "        #compute log-likelihood\n",
    "        log_likelihood = np.log(np.sum(alpha[T-1,:]))\n",
    "        #compute beta\n",
    "        beta = np.zeros([T,K])\n",
    "        beta[T-1,:] = np.ones(shape=(K))\n",
    "        for t in range(T-2,-1,-1):\n",
    "            right_term = np.reshape(B[:,x[t+1]] * beta[t+1,:],\\\n",
    "                               newshape=(K,1))\n",
    "            beta_t = np.dot(A , right_term)#[Kx1]\n",
    "            beta[t,:] = np.reshape(beta_t , newshape=(K))\n",
    "        #ret alpha,beta,log-lik\n",
    "        return alpha,beta,log_likelihood      \n",
    "    \n",
    "    def E_step(self,sentence_in):\n",
    "        \"\"\"Given one observed `sentence_in`, computes sum_chi(i,j), sum_gamma_x(i,j), gamma_1(k).\n",
    "           The notations correspond to numerator of lecture slide 67.\n",
    "           Hint: You can begin by computing alpha and beta as\n",
    "                    `forwards_backwards(self,sentence_in)`\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sentence_in : a list of T words. Each word is a string.\n",
    "                      You can convert sentence_in to a sequence of word-indices\n",
    "                      as `x = self.sentence_to_X(sentence_in)`. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        sum_chi : np.ndarray, shape=(K,K)\n",
    "             Contains values for sum_chi(i,j), numerator of A(i,j) update on slide 67\n",
    "        sum_gamma_x : np.ndarray, shape=(K,V)\n",
    "             Contains values for sum_gamma_x(i,j), numerator of B(i,j) update on slide 67\n",
    "        gamma_1 : np.ndarray, shape=(K,1)\n",
    "             Contains values for gamma_1(k), Pi(k) update on slide 67.\n",
    "        \"\"\"\n",
    "#         alpha,beta,_ = self.forwards_backwards(sentence_in)\n",
    "\n",
    "        \n",
    "        \n",
    "#         X = self.sentence_to_X(sentence_in)\n",
    "        \n",
    "#         V = self.V\n",
    "#         K = self.K\n",
    "#         T= len(X)\n",
    "        \n",
    "#         #calculate gamma, an ndarray of shape (T,K). T=len of sentence, K, no hidden states\n",
    "#         temp = alpha * beta \n",
    "#         gamma = (temp)/np.sum(temp, axis = 1).reshape(-1,1)\n",
    "#         temp.del()\n",
    "        \n",
    "#         #calculate chi, an ndarray of shape (T-1, K,K)\n",
    "#         #we modify the shape of the data involved in the computation. Using the .shape attribute \n",
    "#         #method as it raises an exception if copying of data is required for change of shape\n",
    "#         alpha.shape = (T,K,1)  \n",
    "#         beta.shape = (T,1,K) \n",
    "        \n",
    "#         A = np.reshape (self.model_params.A, (1,K,K))\n",
    "        \n",
    "#         #B is originally of shape (K,V). B sliced is of shape (K,T)\n",
    "#         B_sliced = self.model_params.B[:,X]\n",
    "#         B_sliced = B_sliced.transpose()\n",
    "#         B_sliced.shape = (T,1,K)\n",
    "        \n",
    "#         #old: B = np.reshape (self.model_params.B, (T,1,K))\n",
    "        \n",
    "#         chi_numerator = alpha[:-1,:,:] * A * beta[1:,:,:] * B_sliced[1:,:,:]\n",
    "#         chi = chi_numerator/np.sum(chi_numerator, axis = (1,2)).reshape(-1,1,1)\n",
    "        \n",
    "        \n",
    "#         #create sparse identity matrix, of shape (T,V)\n",
    "#         Id = sp.coo_matrix((np.ones(len(X)),(np.arange(len(X)),X)),shape = (len(X),self.V))\n",
    "#         sum_gamma_x = sp.coo_matrix.dot(gamma.transpose(),Id)\n",
    "#         sum_chi = np.sum(chi,axis = 0)\n",
    "#         gamma_1 = gamma[0].reshape(K,1)\n",
    "        \n",
    "#         return sum_chi,sum_gamma_x,gamma_1\n",
    "\n",
    "            #get A,B,pi ------ \n",
    "        A  = self.model_params.A  #[KxK]\n",
    "        B  = self.model_params.B  #[KxV]\n",
    "        pi = self.model_params.pi #[Kx1] ------ \n",
    "        #compute alpha[T,K],beta[T,K]\n",
    "        alpha,beta, _ = self.forwards_backwards(sentence_in)\n",
    "        T,K = alpha.shape\n",
    "        #sentence -> x\n",
    "        x = self.sentence_to_X(sentence_in)\n",
    "        #compute gamma\n",
    "        prop_gamma = alpha*beta #[T,K]\n",
    "        gamma = prop_gamma/np.reshape(np.sum(prop_gamma,axis=1),newshape=(T,1))\n",
    "        #compute si\n",
    "        si = np.zeros((T-1,K,K))\n",
    "        for t in range(T-1):\n",
    "            i_term = alpha[t,:]\n",
    "            i_term = np.reshape(i_term , newshape=(K,1))\n",
    "            j_term = beta[t+1,:] * B[:,x[t+1]]\n",
    "            j_term = np.reshape(j_term , newshape=(1,K))\n",
    "            prop_si_t = np.dot(i_term,j_term) * A #[KxK]\n",
    "            si[t,:,:] = prop_si_t/np.sum(prop_si_t)\n",
    "        #compute sum_chi \\in [KxK]\n",
    "        sum_chi = np.sum(si,axis=0)\n",
    "        #compute sum_gamma_x \\in [KxV]\n",
    "        V = len(self.word_list)\n",
    "        indic_x = np.zeros([V,T])\n",
    "        indic_x[x,[i for i in range(T)]] = 1\n",
    "        sum_gamma_x = np.dot(indic_x , gamma)#[VxK]\n",
    "        sum_gamma_x = np.transpose(sum_gamma_x)#[KxV]\n",
    "        #compute gamma_1 \\in [Kx1]\n",
    "        gamma_1 = np.reshape(gamma[0,:] , newshape=(K,1))\n",
    "        #ret\n",
    "        return sum_chi, sum_gamma_x, gamma_1\n",
    "        \n",
    "    def categorical (distribution):\n",
    "        \"\"\"super non efficient but did not want to use any other packages\n",
    "        and numpy has only multinomail distribution implementation, no categorical\n",
    "        \"\"\"\n",
    "        temp = np.where(np.random.multinomial(1,distribution) == 1)\n",
    "        return temp[0]\n",
    "    \n",
    "    def generate_sentence(self,sentence_length):\n",
    "        \"\"\" Given the model parameter,generates an observed\n",
    "            sequence of length `sentence_length`.\n",
    "            Hint: after generating a list of word-indices like `x`, you can convert it to\n",
    "                  an actual sentence as `self.X_to_sentence(x)`\n",
    "            \n",
    "        Parameters\n",
    "        ----------\n",
    "        sentence_length : int,\n",
    "                        length of the generated sentence.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        sent : a list of words, like ['the' , 'food' , 'was' , 'good'] \n",
    "               a sentence generated from the model.\n",
    "        \"\"\"\n",
    "\n",
    "        #first generate the markov chain\n",
    "        \n",
    "        Z = np.zeros(sentence_length)\n",
    "        Z[0]=categorical(self.model_params.pi)\n",
    "        for t in range(1,sentence_length):\n",
    "            Z[t] = categorical(self.model_params.A[Z[t-1]])\n",
    "            \n",
    "        #now generate the \"seen\" chain, ie X\n",
    "        X = np.zeros(sentence_length)\n",
    "        for t in range(sentence_length):\n",
    "            X[t] = categorical(self.model_params.B[Z[t]])\n",
    "        \n",
    "    \n",
    "    def X_to_sentence(self,input_x):\n",
    "        \"\"\"Convert a list of word-indices to an actual sentence (i.e. a list of words).\n",
    "           To convert a word-index to an actual word, it looks at `self.word_list`.\n",
    "           \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_x : a list of integer\n",
    "                  list of word-indices, like [0,6,1,3,2,...,1]\n",
    "        \n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        sent : a list of words like ['the', 'food', 'was', 'good']\n",
    "        \"\"\"\n",
    "        sent = []\n",
    "        V = len(self.word_list)\n",
    "        for u in input_x:\n",
    "            if(u<V):\n",
    "                sent.append(self.word_list[u])\n",
    "            else:\n",
    "                raise Exception(\"values of input_x have to be in \" +\\\n",
    "                                str([0,V-1])  + \", but got the value \" + str(u) + \".\")\n",
    "        return sent\n",
    "    \n",
    "    def sentence_to_X(self,input_sentence):\n",
    "        \"\"\"Convert a sentence (i.e. a list of words) to a list of word-indices.\n",
    "           Index of the word `w` is `self.word_list.index(w)`.\n",
    "           \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sentence : list\n",
    "                         a list of words like ['the', 'food', 'was', 'good']\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X : list\n",
    "            a list of word-indices like [50,4,3,20]\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        for w in input_sentence:\n",
    "            X.append(self.word_list.index(w))\n",
    "        return X\n",
    "    \n",
    "    def is_in_vocab(self,sentence_in):\n",
    "        \"\"\"Checks if all words in sentence_in are in vocabulary.\n",
    "           If `sentence_in` contains a word like `w` which is not in `self.word_list`,\n",
    "           it means that we've not seen word `w` in training set (i.e. `curpus`).\n",
    "           \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        sentence_in : list\n",
    "                      a list of words like ['the', 'food', 'was', 'good']\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        to_ret : boolean\n",
    "            [We've seen all words in `sentence_in` when training model-params.]\n",
    "        \"\"\"\n",
    "        to_return = True\n",
    "        for w in sentence_in:\n",
    "            if(w not in self.word_list):\n",
    "                to_return = False\n",
    "        return to_return\n",
    "    \n",
    "    def update_params(self):\n",
    "        \"\"\" One update procedure of the EM algorithm.\n",
    "            - E-step: For each sentence like `sent` in corpus, it firstly computes gammas and chis. \n",
    "                    Then, it sums them up to obtain numerators for M-step (slide 67).\n",
    "            - M-step: normalize values obtain in E-step and assign new values to A, B, pi.\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        #E-step\n",
    "        K = self.K\n",
    "        V = self.V\n",
    "        corpus_sum_chi , corpus_sum_gamma_x , corpus_gamma_1 = np.zeros([K,K]),np.zeros([K,V]),np.zeros([K,1])\n",
    "        for sent in self.corpus[0:10]:\n",
    "            sent_sum_chi, sent_sum_gamma_x, sent_gamma_1 = self.E_step(sent)\n",
    "            corpus_sum_chi += sent_sum_chi\n",
    "            corpus_sum_gamma_x += sent_sum_gamma_x\n",
    "            corpus_gamma_1 += sent_gamma_1\n",
    "        #M-step\n",
    "        A_new  = corpus_sum_chi / np.reshape(np.sum(corpus_sum_chi,axis=1),newshape=(K,1))\n",
    "        B_new  = corpus_sum_gamma_x / np.reshape(np.sum(corpus_sum_gamma_x,axis=1),newshape=(K,1))\n",
    "        pi_new = corpus_gamma_1 / np.sum(corpus_gamma_1)\n",
    "        self.model_params.A = A_new\n",
    "        self.model_params.B = B_new\n",
    "        self.model_params.pi = pi_new\n",
    "        \n",
    "        #debug section:\n",
    "        #print(\"A_new : %s\"%str(A_new))\n",
    "        #print(\"B_new : %s\"%str(B_new))\n",
    "        #print(\"pi_new : %s\"%str(pi_new))\n",
    "    \n",
    "    def learn_params(self,num_iter):\n",
    "        \"\"\" Runs update procedures of the EM-algorithm for `num_iter` iterations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_iter: int\n",
    "                  number of iterations.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        history_loglik: list of floats\n",
    "                `history_loglik[t]` is log-probability of training data in iteration `t`.\n",
    "        \"\"\"\n",
    "        history_loglik = []\n",
    "        for counter in range(num_iter):\n",
    "            print(\"iteration \" + str(counter) +\\\n",
    "                  \" of \" + str(num_iter) , end=\"\\r\")\n",
    "            history_loglik.append(self.loglik_corpus())\n",
    "            self.update_params()\n",
    "        return history_loglik\n",
    "    \n",
    "    def loglik_corpus(self):\n",
    "        \"\"\" Computes log-likelihood of the corpus based on current parameters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loglik: float\n",
    "                log-likelihood of the corpus based on current parameters.\n",
    "        \n",
    "        \"\"\"\n",
    "        loglik = 0\n",
    "        for sent in self.corpus:\n",
    "            _,_,loglik_of_sent = self.forwards_backwards(sent)\n",
    "            loglik += loglik_of_sent\n",
    "        return loglik\n",
    "    \n",
    "    def loglik_sentence(self,sentence_in):\n",
    "        \"\"\" Computes log-likelihood of `sentence_in` based on current parameters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sentence_in: a list of words\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loglik_of_sent: float\n",
    "                        log-likelihood of `sentence_in` based on current parameters.\n",
    "        \"\"\"\n",
    "        #check if all words are in corpus.\n",
    "        for w in sentence_in:\n",
    "            if(w not in self.word_list):\n",
    "                return -np.Inf\n",
    "        _,_,loglik_of_sent = self.forwards_backwards(sentence_in)\n",
    "        return loglik_of_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split each set of reviews (i.e. `reviews_1star` and `reviews_5star`) into training/test sets.\n",
    "You can change percentage of train/test instances by setting `train_percentage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 80\n",
    "def split_to_traintest(in_list,percentage):\n",
    "    n_train = math.floor(len(in_list)*percentage/100)\n",
    "    n_test  = len(in_list) - n_train\n",
    "    return in_list[0:n_train],in_list[n_train:]\n",
    "reviews_1star_train , reviews_1star_test = split_to_traintest(reviews_1star,train_percentage)\n",
    "reviews_5star_train , reviews_5star_test = split_to_traintest(reviews_5star,train_percentage)\n",
    "reviews_test = reviews_1star_test + reviews_5star_test\n",
    "y_test  = [1 for i in range(len(reviews_1star_test))] + \\\n",
    "          [5 for i in range(len(reviews_5star_test))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that 1-star reviews and 5-star reviews are generated from two different HMMs.\n",
    "Therefore, we define two HMMs `hmm_1star` and `hmm_5star` and we learn their parameters using the sets `reviews_1star_train` and `reviews_5star_train`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly let's define two HMMs. `K` is the number of possible hidden states. Here we set it maually to a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 8\n",
    "hmm_1 = HMM_TxtGenerator(reviews_1star_train,K)\n",
    "hmm_5 = HMM_TxtGenerator(reviews_5star_train,K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit the HMMs to their corresponding training sets using expectationâ€“maximization (EM) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `HMM_TxtGenerator.learn_params(num_iter)` repeats EM steps for some iterations, and returns the history of log-likelihood during the steps.\n",
    "At the following, we repeat EM updates for `n_iter` iterations and plot the history of log-likelihood. When log-probability of evidence stops increasing, it means that we can quit EM updates. Please note that log-likelihood might increase dramatically in first iterations. Therefore, in the plot the amount of increase in final iterations would look small. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can increase/decrease the number of iterations by setting `n_iter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 of 10\r",
      "iteration 1 of 10\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tudor/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:99: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9 of 10\r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_iter = 10\n",
    "history_loglik_1 = hmm_1.learn_params(n_iter)\n",
    "plt.figure()\n",
    "plt.plot(range(len(history_loglik_1)) , history_loglik_1)\n",
    "plt.xlabel(\"iteration\",fontsize=16)\n",
    "plt.ylabel(\"log-likelihood\",fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 of 2\r"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 't' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-24c221901978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory_loglik_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmm_5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_loglik_5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mhistory_loglik_5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iteration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log-likelihood\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-a601a6a565f9>\u001b[0m in \u001b[0;36mlearn_params\u001b[0;34m(self, num_iter)\u001b[0m\n\u001b[1;32m    301\u001b[0m             print(\"iteration \" + str(counter) +\\\n\u001b[1;32m    302\u001b[0m                   \" of \" + str(num_iter) , end=\"\\r\")\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0mhistory_loglik\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglik_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory_loglik\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-a601a6a565f9>\u001b[0m in \u001b[0;36mloglik_corpus\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mloglik\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloglik_of_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwards_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0mloglik\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloglik_of_sent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloglik\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-a601a6a565f9>\u001b[0m in \u001b[0;36mforwards_backwards\u001b[0;34m(self, sentence_in)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mbeta\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'debug. shapes of pi = %s , B[:,x[0]] = %s, alpha[0] = %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 't' referenced before assignment"
     ]
    }
   ],
   "source": [
    "history_loglik_5 = hmm_5.learn_params(n_iter)\n",
    "plt.figure()\n",
    "plt.plot(range(len(history_loglik_5)) , history_loglik_5)\n",
    "plt.xlabel(\"iteration\",fontsize=16)\n",
    "plt.ylabel(\"log-likelihood\",fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have learned the parameters $\\pi_1,\\mathbf{A}_1,\\mathbf{B}_1$ and $\\pi_5,\\mathbf{A}_5,\\mathbf{B}_5$. Assume that we have a sentence of length $T$ and we want to classify it as either 1-star or 5-star. For this task, we consider the following probabilistic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename =\"task03_classification.png\" , width=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above model, $Y\\in \\lbrace 1,5 \\rbrace$ indicates the class of the review, either 1-star or 5-star. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative process of the above model is as follows:\n",
    "* Generate the class of the sentence:\n",
    "\\begin{equation}\n",
    "Pr(Y=1) = p \\;\\;\\;\\;, \\;\\;\\;\\; Pr(Y=5) = 1-p \\;\\;.\n",
    "\\end{equation}\n",
    "* If 1-star (resp. 5-star) category is chosen, generate the sentence from marginal distribution of HMM 1 (resp. 5):\n",
    "\\begin{equation}\n",
    "Pr(X_{1:T} | Y) = \n",
    "\\begin{cases}\n",
    "Pr(X_{1:T}|\\pi_1,\\mathbf{A}_1,\\mathbf{B}_1) \\;\\;\\;\\;\\;\\; Y=1 \\\\\n",
    "Pr(X_{1:T}|\\pi_5,\\mathbf{A}_5,\\mathbf{B}_5) \\;\\;\\;\\;\\;\\; Y=5\n",
    "\\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify a sentence $X_{1:T}$, we can simply compare $Pr(Y=1|X_{1:T})$ with $Pr(Y=5|X_{1:T})$. Using Bayes' theorem, we can make decision by computing the terms\n",
    "$logPr(Y=1)+logPr(X_{1:T}|\\pi_1,\\mathbf{A}_1,\\mathbf{B}_1)$ and $logPr(Y=5)+ logPr(X_{1:T}|\\pi_5,\\mathbf{A}_5,\\mathbf{B}_5)$. More precisely:\n",
    "* The terms $logPr(X_{1:T}|\\pi_1,\\mathbf{A}_1,\\mathbf{B}_1)$ and $logPr(X_{1:T}|\\pi_5,\\mathbf{A}_5,\\mathbf{B}_5)$ are provided by the functions `hmm_1.loglik_sentence` and `hmm_5.loglik_sentence`. \n",
    "* We can approximate $Pr(Y=1)$ and $Pr(Y=5)$ simply by computing the frequency of each class in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that if a sentence contains a word which is **not** in the vocabulary of a model, likelihood of the sentence is zero and log-likelihood is $-\\infty$. At the following, we drop the samples in test set which have $-\\infty$ log-likelihood in both models. We store all other samples in `reviews_test_filtered` and their categories in `y_test_filtered`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_reviews = []\n",
    "temp_y = []\n",
    "for counter in range(len(reviews_test)):\n",
    "    current_review = reviews_test[counter]\n",
    "    current_y   = y_test[counter]\n",
    "    if(hmm_1.is_in_vocab(current_review) | hmm_5.is_in_vocab(current_review)):\n",
    "        temp_reviews.append(current_review)\n",
    "        temp_y.append(current_y)\n",
    "reviews_test_filtered = temp_reviews\n",
    "y_test_filtered = temp_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `classify_review` which is expalined at the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(hmm_1,hmm_5,p,sentence_in):\n",
    "    \"\"\"Given the trained models `hmm_1` and `hmm_2` and frequency of\n",
    "       1-star reviews, classifies `sentence_in` \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hmm_1 : HMM_TxtGenerator\n",
    "        The trained model on 1-star reviews.\n",
    "    hmm_5 : HMM_TxtGenerator\n",
    "        The trained model on 5-star reviews.\n",
    "    p: a scalar in [0,1]\n",
    "        frequency of 1-star reviews, (#1star)/(#1star + #5star)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    c : int in {1,5}\n",
    "        c=1 means sentence_in is classified as 1. \n",
    "        similarly c=5 means sentence_in is classified as 5.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate our classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = len(reviews_1star_train)/(len(reviews_1star_train)+len(reviews_5star_train))\n",
    "y_pred = []\n",
    "for sent in reviews_test_filtered:\n",
    "    y_pred.append(classify_review(hmm_1,hmm_5,p,sent))\n",
    "accuracy = np.sum(np.array(y_pred)==np.array(y_test_filtered))/len(y_test_filtered)\n",
    "print(\"classification accuracy for \" + str(len(y_test_filtered)) +\\\n",
    "      \" test instances: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can investigate how the generated sentences would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_1star = hmm_1.generate_sentence(15)\n",
    "sample_5star = hmm_5.generate_sentence(15)\n",
    "print(\"generated 1star review: \")\n",
    "print(sample_1star)\n",
    "print(\"\\n\")\n",
    "print(\"generated 5star review: \")\n",
    "print(sample_5star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
